import sys
sys.path.append('.')

import os
import json
import logging
import asyncio
import sqlite3
import queue
import re
import random
from datetime import datetime
from dataclasses import dataclass
from typing import Dict, List, Optional
from fastapi import FastAPI, HTTPException, Response, UploadFile, File, Form
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
from pathlib import Path
from groq import AsyncGroq
from dotenv import load_dotenv
import whisper
from gtts import gTTS
import tempfile

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('debug.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ëª¨ë“ˆí™”ëœ êµ¬ì¡° import
from mbti_analyzer.core.analyzer import analyze_tf_tendency, generate_f_friendly_response, get_f_friendly_alternatives, get_t_strong_ment, get_t_mild_ment
from mbti_analyzer.core.question_generator import generate_ai_questions_real, generate_fallback_questions, generate_ai_questions
from mbti_analyzer.core.final_analyzer import generate_final_analysis
from mbti_analyzer.modules.stt_module import transcribe_audio_file
from mbti_analyzer.modules.tts_module import text_to_speech
from mbti_analyzer.modules.stt_module_enhanced import transcribe_audio_file_enhanced, validate_audio_quality
from mbti_analyzer.modules.sentence_correction_enhanced import correct_sentence_with_ai_enhanced

# ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œì„ ìœ„í•œ ë°ì´í„° í´ë˜ìŠ¤ë“¤
@dataclass
class UserInput:
    """ì‚¬ìš©ì ì…ë ¥ ë°ì´í„°"""
    question: str
    answer: str
    expected_score: float
    actual_score: float
    timestamp: datetime
    error: float
    is_acceptable: bool

@dataclass
class PromptVersion:
    """í”„ë¡¬í”„íŠ¸ ë²„ì „ ì •ë³´"""
    version: str
    prompt: str
    performance: Dict
    created_at: datetime

class LearningFeedbackRequest(BaseModel):
    question: str
    answer: str
    expected_score: float
    actual_score: float

class LearningStatusResponse(BaseModel):
    enabled: bool
    total_inputs: int
    acceptable_rate: float
    average_error: float
    current_version: str

# ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
class RealtimeLearningManager:
    def __init__(self):
        self.user_inputs_queue = queue.Queue()
        self.current_prompt_version = "v1.0"
        self.prompt_history = []
        self.performance_threshold = 0.6  # 60% í—ˆìš© ì˜¤ì°¨ ë‹¬ì„± ì‹œ ê°œì„ 
        self.min_inputs_for_tuning = 10   # íŠœë‹ì„ ìœ„í•œ ìµœì†Œ ì…ë ¥ ìˆ˜
        self.db_path = "learning_data.db"
        self.learning_enabled = True
        self.init_database()
        
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ì‚¬ìš©ì ì…ë ¥ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS user_inputs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                question TEXT,
                answer TEXT,
                expected_score REAL,
                actual_score REAL,
                error REAL,
                is_acceptable BOOLEAN,
                timestamp DATETIME,
                prompt_version TEXT
            )
        ''')
        
        # í”„ë¡¬í”„íŠ¸ ë²„ì „ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS prompt_versions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                version TEXT,
                prompt TEXT,
                performance_data TEXT,
                created_at DATETIME
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def calculate_error(self, expected: float, actual: float) -> float:
        """ì˜¤ì°¨ ê³„ì‚°"""
        return abs(expected - actual)
    
    def is_acceptable_error(self, error: float) -> bool:
        """ì˜¤ì°¨ê°€ í—ˆìš© ë²”ìœ„(10%) ë‚´ì¸ì§€ í™•ì¸"""
        return error <= 10.0
    
    async def process_user_input(self, question: str, answer: str, expected_score: float, actual_score: float) -> Dict:
        """ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° í•™ìŠµ"""
        if not self.learning_enabled:
            return {"success": True, "learning_disabled": True}
        
        print(f"ğŸ”„ ì‹¤ì‹œê°„ í•™ìŠµ: ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ì¤‘...")
        print(f"ì§ˆë¬¸: {question}")
        print(f"ë‹µë³€: {answer}")
        print(f"ì˜ˆìƒ: {expected_score}%, ì‹¤ì œ: {actual_score}%")
        
        error = self.calculate_error(expected_score, actual_score)
        is_acceptable = self.is_acceptable_error(error)
        
        # ì‚¬ìš©ì ì…ë ¥ ì €ì¥
        user_input = UserInput(
            question=question,
            answer=answer,
            expected_score=expected_score,
            actual_score=actual_score,
            timestamp=datetime.now(),
            error=error,
            is_acceptable=is_acceptable
        )
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        self.save_user_input(user_input)
        
        # íì— ì¶”ê°€
        self.user_inputs_queue.put(user_input)
        
        # ì„±ëŠ¥ í‰ê°€ ë° í•„ìš”ì‹œ íŠœë‹
        await self.evaluate_and_tune_if_needed()
        
        status_emoji = "âœ…" if is_acceptable else "âŒ"
        print(f"{status_emoji} í•™ìŠµ ê²°ê³¼: ì˜¤ì°¨ {error:.1f}%")
        
        return {
            "success": True,
            "error": error,
            "is_acceptable": is_acceptable,
            "prompt_version": self.current_prompt_version
        }
    
    def save_user_input(self, user_input: UserInput):
        """ì‚¬ìš©ì ì…ë ¥ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO user_inputs 
            (question, answer, expected_score, actual_score, error, is_acceptable, timestamp, prompt_version)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            user_input.question,
            user_input.answer,
            user_input.expected_score,
            user_input.actual_score,
            user_input.error,
            user_input.is_acceptable,
            user_input.timestamp.isoformat(),
            self.current_prompt_version
        ))
        
        conn.commit()
        conn.close()
    
    def get_recent_performance(self, limit: int = 50) -> Dict:
        """ìµœê·¼ ì„±ëŠ¥ ë°ì´í„° ì¡°íšŒ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT expected_score, actual_score, error, is_acceptable
            FROM user_inputs
            ORDER BY timestamp DESC
            LIMIT ?
        ''', (limit,))
        
        results = cursor.fetchall()
        conn.close()
        
        if not results:
            return {
                "total_inputs": 0,
                "acceptable_rate": 0,
                "average_error": 0
            }
        
        total_inputs = len(results)
        acceptable_count = sum(1 for _, _, _, is_acceptable in results if is_acceptable)
        total_error = sum(error for _, _, error, _ in results)
        
        return {
            "total_inputs": total_inputs,
            "acceptable_rate": (acceptable_count / total_inputs) * 100,
            "average_error": total_error / total_inputs
        }
    
    async def evaluate_and_tune_if_needed(self):
        """ì„±ëŠ¥ í‰ê°€ ë° í•„ìš”ì‹œ íŠœë‹"""
        performance = self.get_recent_performance()
        
        print(f"ğŸ“Š í˜„ì¬ ì„±ëŠ¥:")
        print(f"   - ì´ ì…ë ¥: {performance['total_inputs']}ê°œ")
        print(f"   - í—ˆìš© ë¹„ìœ¨: {performance['acceptable_rate']:.1f}%")
        print(f"   - í‰ê·  ì˜¤ì°¨: {performance['average_error']:.1f}%")
        
        # íŠœë‹ ì¡°ê±´ í™•ì¸
        if (performance['total_inputs'] >= self.min_inputs_for_tuning and 
            performance['acceptable_rate'] < self.performance_threshold * 100):
            
            print(f"âš ï¸  ì„±ëŠ¥ ê°œì„  í•„ìš”! ìë™ íŠœë‹ ì‹œì‘...")
            await self.auto_tune_prompt()
    
    async def auto_tune_prompt(self):
        """ìë™ í”„ë¡¬í”„íŠ¸ íŠœë‹"""
        print(f"ğŸ”§ í”„ë¡¬í”„íŠ¸ ìë™ íŠœë‹ ì¤‘...")
        
        # ìµœê·¼ ì…ë ¥ ë°ì´í„° ë¶„ì„
        recent_inputs = self.get_recent_user_inputs(20)
        
        # ì˜¤ì°¨ íŒ¨í„´ ë¶„ì„
        error_patterns = self.analyze_error_patterns(recent_inputs)
        
        # í”„ë¡¬í”„íŠ¸ ê°œì„ 
        improved_prompt = self.generate_improved_prompt(error_patterns)
        
        # ìƒˆ ë²„ì „ ì €ì¥
        new_version = f"v{len(self.prompt_history) + 1}.0"
        self.save_prompt_version(new_version, improved_prompt)
        
        print(f"âœ… í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {new_version}")
        
        self.current_prompt_version = new_version
    
    def get_recent_user_inputs(self, limit: int = 20) -> List[UserInput]:
        """ìµœê·¼ ì‚¬ìš©ì ì…ë ¥ ì¡°íšŒ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT question, answer, expected_score, actual_score, error, is_acceptable, timestamp
            FROM user_inputs
            ORDER BY timestamp DESC
            LIMIT ?
        ''', (limit,))
        
        results = cursor.fetchall()
        conn.close()
        
        return [
            UserInput(
                question=row[0],
                answer=row[1],
                expected_score=row[2],
                actual_score=row[3],
                timestamp=datetime.fromisoformat(row[6]),
                error=row[4],
                is_acceptable=bool(row[5])
            )
            for row in results
        ]
    
    def analyze_error_patterns(self, inputs: List[UserInput]) -> Dict:
        """ì˜¤ì°¨ íŒ¨í„´ ë¶„ì„"""
        high_errors = [i for i in inputs if i.error > 10]
        medium_errors = [i for i in inputs if 5 < i.error <= 10]
        low_errors = [i for i in inputs if i.error <= 5]
        
        # T ì„±í–¥ ê³¼ì†Œ í‰ê°€ íŒ¨í„´
        t_underestimated = [i for i in high_errors if i.actual_score < i.expected_score - 10]
        
        # F ì„±í–¥ ê³¼ì†Œ í‰ê°€ íŒ¨í„´
        f_underestimated = [i for i in high_errors if i.actual_score > i.expected_score + 10]
        
        return {
            "high_errors": high_errors,
            "medium_errors": medium_errors,
            "low_errors": low_errors,
            "t_underestimated": t_underestimated,
            "f_underestimated": f_underestimated
        }
    
    def generate_improved_prompt(self, error_patterns: Dict) -> str:
        """ì˜¤ì°¨ íŒ¨í„´ì„ ë°”íƒ•ìœ¼ë¡œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        base_prompt = """
        MBTI T/F ì„±í–¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ T/F ì„±í–¥ì„ í‰ê°€í•˜ì„¸ìš”.

        [ë¶„ì„ ê¸°ì¤€]
        - T(Thinking): ë…¼ë¦¬ì , ê°ê´€ì , ë¶„ì„ì  ì‚¬ê³ , ì›ì¸ ë¶„ì„, ì²´ê³„ì  ì ‘ê·¼, íš¨ìœ¨ì„± ì¤‘ì‹œ, ë¬¸ì œ í•´ê²° ì§€í–¥
        - F(Feeling): ê°ì •ì , ê³µê°ì , ê´€ê³„ ì¤‘ì‹¬ì  ì‚¬ê³ , ê¸°ë¶„ ê³ ë ¤, ê³µê° í‘œí˜„, ê´€ê³„ ì¤‘ì‹œ, ê°ì •ì  ì§€ì§€
        - ì ìˆ˜: 0(ë§¤ìš° ê°•í•œ T) ~ 100(ë§¤ìš° ê°•í•œ F), 50=ê· í˜•

        [í•µì‹¬ ë¶„ì„ ì›ì¹™]
        1. ë‹µë³€ì˜ ì£¼ìš” ì˜ë„ì™€ í•µì‹¬ ë©”ì‹œì§€ì— ì§‘ì¤‘
        2. T ì„±í–¥ ê°•í•œ í‘œí˜„: "ë¶„ì„", "ì›ì¸", "ë…¼ë¦¬", "ì²´ê³„", "íš¨ìœ¨", "ë°©ì§€", "íŒŒì•…", "ê²°ê³¼", "í•´ê²°", "ì ‘ê·¼", "ë‹¨ê³„ë³„", "ì²´ê³„ì "
        3. F ì„±í–¥ ê°•í•œ í‘œí˜„: "ê¸°ë¶„", "ë§ˆìŒ", "ê³µê°", "í˜ë“¤", "ì•ˆíƒ€ê¹", "ê¶ê¸ˆ", "ë„ì™€", "í•¨ê»˜", "ì§€ì§€", "ìœ„ë¡œ", "ê±±ì •", "ì•ˆíƒ€ê¹"
        4. í˜¼í•© ë‹µë³€ ë¶„ì„: 
           - T+F í˜¼í•© ë‹µë³€ì˜ ê²½ìš°: í•µì‹¬ ë©”ì‹œì§€ì˜ ë°©í–¥ì„±ì— ë”°ë¼ íŒë‹¨
           - "ë¶„ì„í•˜ì" + "ìì±…í•˜ì§€ ë§ˆ" â†’ T ì„±í–¥ì´ ìš°ì„  (40-60ì )
           - "í•¨ê»˜ ìƒê°í•´ë³´ì" â†’ F ì„±í–¥ì´ ìš°ì„  (60-80ì )
        5. ë§¥ë½ë³„ ì ìˆ˜ ê°€ì´ë“œ:
           - ìˆœìˆ˜ T ì„±í–¥ (ë…¼ë¦¬ì  í•´ê²°): 20-40ì 
           - T+F í˜¼í•© (ë¶„ì„+ê³µê°): 40-70ì   
           - ìˆœìˆ˜ F ì„±í–¥ (ê°ì •ì  ì§€ì§€): 70-90ì 

        [ì¶œë ¥ í˜•ì‹]
        [ë¶„ì„] ë‹µë³€ìì˜ T/F ì„±í–¥ ë¶„ì„ (ì„±í–¥ ê°•ë„ì™€ ì£¼ìš” íŠ¹ì§• ëª…ì‹œ)
        [ê·¼ê±°] ë¶„ì„ ê·¼ê±° (êµ¬ì²´ì  í‚¤ì›Œë“œì™€ í‘œí˜„ ë°©ì‹, ì˜ë„ íŒŒì•…)
        [ì œì•ˆ] ê°œì„  ì œì•ˆ 3ê°€ì§€
        [ëŒ€ì•ˆ] ëŒ€ì•ˆ ë‹µë³€
        ì ìˆ˜: X

        ë‹µë³€: {text}
        """
        
        # ì˜¤ì°¨ íŒ¨í„´ì— ë”°ë¥¸ ê°œì„ ì‚¬í•­ ì¶”ê°€
        improvements = []
        
        if error_patterns["t_underestimated"]:
            improvements.append("âš ï¸ T ì„±í–¥ ê³¼ì†Œ í‰ê°€ ë¬¸ì œ ë°œê²¬ - T ì„±í–¥ ë¶„ì„ ê°•í™” í•„ìš”")
            base_prompt += "\n[íŠ¹ë³„ ì£¼ì˜ì‚¬í•­]\n- T ì„±í–¥ í‘œí˜„ì´ ìˆëŠ” ê²½ìš° ê³¼ì†Œ í‰ê°€í•˜ì§€ ë§ ê²ƒ\n- 'ë…¼ë¦¬ì ', 'ì²´ê³„ì ', 'ë¶„ì„ì ' í‘œí˜„ ì‹œ T ì ìˆ˜ ë†’ê²Œ ë¶€ì—¬"
        
        if error_patterns["f_underestimated"]:
            improvements.append("âš ï¸ F ì„±í–¥ ê³¼ì†Œ í‰ê°€ ë¬¸ì œ ë°œê²¬ - F ì„±í–¥ ë¶„ì„ ê°•í™” í•„ìš”")
            base_prompt += "\n- F ì„±í–¥ í‘œí˜„ì´ ìˆëŠ” ê²½ìš° ê³¼ì†Œ í‰ê°€í•˜ì§€ ë§ ê²ƒ\n- 'ê³µê°', 'ì§€ì§€', 'ìœ„ë¡œ' í‘œí˜„ ì‹œ F ì ìˆ˜ ë†’ê²Œ ë¶€ì—¬"
        
        return base_prompt
    
    def save_prompt_version(self, version: str, prompt: str):
        """í”„ë¡¬í”„íŠ¸ ë²„ì „ ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        performance = self.get_recent_performance()
        
        cursor.execute('''
            INSERT INTO prompt_versions 
            (version, prompt, performance_data, created_at)
            VALUES (?, ?, ?, ?)
        ''', (
            version,
            prompt,
            json.dumps(performance),
            datetime.now().isoformat()
        ))
        
        conn.commit()
        conn.close()
        
        self.prompt_history.append(PromptVersion(
            version=version,
            prompt=prompt,
            performance=performance,
            created_at=datetime.now()
        ))

# ì „ì—­ í•™ìŠµ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
learning_manager = RealtimeLearningManager()

# ëª¨ë“ˆ ì—°ê²° ìƒíƒœ ë¡œê¹…
print("=== MBTI T/F Analyzer ëª¨ë“ˆ ì—°ê²° ìƒíƒœ í™•ì¸ ===")
logger.info("=== MBTI T/F Analyzer ëª¨ë“ˆ ì—°ê²° ìƒíƒœ í™•ì¸ ===")
print("âœ… ë¶„ì„ê¸° ëª¨ë“ˆ (mbti_analyzer.core.analyzer) - ì—°ê²°ë¨")
logger.info("âœ… ë¶„ì„ê¸° ëª¨ë“ˆ (mbti_analyzer.core.analyzer) - ì—°ê²°ë¨")
print("âœ… ì§ˆë¬¸ ìƒì„±ê¸° ëª¨ë“ˆ (mbti_analyzer.core.question_generator) - ì—°ê²°ë¨")
logger.info("âœ… ì§ˆë¬¸ ìƒì„±ê¸° ëª¨ë“ˆ (mbti_analyzer.core.question_generator) - ì—°ê²°ë¨")
print("âœ… ìµœì¢… ë¶„ì„ê¸° ëª¨ë“ˆ (mbti_analyzer.core.final_analyzer) - ì—°ê²°ë¨")
logger.info("âœ… ìµœì¢… ë¶„ì„ê¸° ëª¨ë“ˆ (mbti_analyzer.core.final_analyzer) - ì—°ê²°ë¨")
print("âœ… STT ëª¨ë“ˆ (mbti_analyzer.modules.stt_module) - ì—°ê²°ë¨")
logger.info("âœ… STT ëª¨ë“ˆ (mbti_analyzer.modules.stt_module) - ì—°ê²°ë¨")
print("âœ… TTS ëª¨ë“ˆ (mbti_analyzer.modules.tts_module) - ì—°ê²°ë¨")
logger.info("âœ… TTS ëª¨ë“ˆ (mbti_analyzer.modules.tts_module) - ì—°ê²°ë¨")
print("=== ëª¨ë“  ëª¨ë“ˆ ì—°ê²° ì™„ë£Œ ===")
logger.info("=== ëª¨ë“  ëª¨ë“ˆ ì—°ê²° ì™„ë£Œ ===")

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
    expose_headers=["*"]
)

# ëª¨ë¸ ì´ˆê¸°í™”
print("Starting MBTI T/F Analyzer...")

# AI ëª¨ë¸ ì´ˆê¸°í™” (Gemini 1ìˆœìœ„, Groq 2ìˆœìœ„)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")

# Gemini ì´ˆê¸°í™”
if GEMINI_API_KEY:
    import google.generativeai as genai
    genai.configure(api_key=GEMINI_API_KEY)
    GEMINI_MODEL = genai.GenerativeModel('gemini-1.5-pro-latest')
    print("âœ… Gemini AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (1ìˆœìœ„)", flush=True)
else:
    GEMINI_MODEL = None
    print("âš ï¸ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ.", flush=True)

# Groq ì´ˆê¸°í™” (ë°±ì—…ìš©)
if GROQ_API_KEY:
    AI_CLIENT = AsyncGroq(api_key=GROQ_API_KEY)
    print("âœ… Groq AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (2ìˆœìœ„)", flush=True)
else:
    AI_CLIENT = None
    print("âš ï¸ GROQ_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ.", flush=True)

# STT ëª¨ë¸ ì´ˆê¸°í™”
print("Loading Whisper model...")
whisper_model = whisper.load_model("base")
print("Whisper model loaded successfully!")

# ì •ì  íŒŒì¼ë“¤ì„ ì„œë¹„ìŠ¤
app.mount("/static", StaticFiles(directory="."), name="static")
app.mount("/Main_pg", StaticFiles(directory="Main_pg"), name="mainpg")
app.mount("/images", StaticFiles(directory="images"), name="images")
app.mount("/fonts", StaticFiles(directory="fonts"), name="fonts")
app.mount("/Final", StaticFiles(directory="Final"), name="final")
app.mount("/Question_pg", StaticFiles(directory="Question_pg"), name="questionpg")

# ë£¨íŠ¸ ê²½ë¡œì—ì„œ HTML íŒŒì¼ë“¤ ì œê³µ
from fastapi.responses import FileResponse

@app.get("/")
async def read_index():
    logger.info("ğŸ” ë£¨íŠ¸ í˜ì´ì§€ ìš”ì²­ ì²˜ë¦¬ ì¤‘...")
    try:
        response = FileResponse("index1.html")
        logger.info("âœ… ë£¨íŠ¸ í˜ì´ì§€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë¨")
        return response
    except Exception as e:
        logger.error(f"âŒ ë£¨íŠ¸ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/index1.html")
async def read_index1():
    return FileResponse("index1.html")

@app.get("/index2.html")
async def read_index2():
    return FileResponse("index2.html")

@app.get("/index3.html")
async def read_index3():
    return FileResponse("index3.html")

@app.get("/answer.html")
async def read_answer():
    return FileResponse("answer.html")

@app.get("/common.css")
async def read_css():
    return FileResponse("common.css")

@app.get("/favicon.ico")
async def get_favicon():
    """
    ë¸Œë¼ìš°ì €ê°€ ìë™ìœ¼ë¡œ ìš”ì²­í•˜ëŠ” favicon.icoì— ëŒ€í•œ ì‘ë‹µ
    """
    return Response(content="", media_type="image/x-icon")

class TextRequest(BaseModel):
    text: str

class SentenceCorrectionRequest(BaseModel):
    text: str

class DetailedAnalysisRequest(BaseModel):
    question: str
    answer: str
    score: float

class AnalysisResponse(BaseModel):
    score: float
    detailed_analysis: Optional[str] = None
    reasoning: Optional[str] = None
    suggestions: Optional[list] = None
    alternative_response: Optional[str] = None

class FinalAnalysisRequest(BaseModel):
    results: List[Dict]  # [{question, answer, score}, ...]

class FinalAnalysisResponse(BaseModel):
    overall_tendency: str
    personality_analysis: str
    communication_strategy: str
    strengths: List[str]
    growth_areas: List[str]
    keyword_analysis: Dict[str, Dict[str, int]]  # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì‚¬ìš© íšŸìˆ˜

class QuestionGenerationRequest(BaseModel):
    count: Optional[int] = 5  # ìƒì„±í•  ì§ˆë¬¸ ê°œìˆ˜
    difficulty: Optional[str] = "medium"  # easy, medium, hard

# generate_ai_questions_real, generate_fallback_questions, generate_ai_questions í•¨ìˆ˜ëŠ” mbti_analyzer.core.question_generatorì—ì„œ importí•˜ì—¬ ì‚¬ìš©

# analyze_tf_tendency í•¨ìˆ˜ëŠ” mbti_analyzer.core.analyzerì—ì„œ importí•˜ì—¬ ì‚¬ìš©

# generate_f_friendly_responseì™€ get_f_friendly_alternatives í•¨ìˆ˜ëŠ” mbti_analyzer.core.analyzerì—ì„œ importí•˜ì—¬ ì‚¬ìš©

# generate_final_analysis í•¨ìˆ˜ëŠ” mbti_analyzer.core.final_analyzerì—ì„œ importí•˜ì—¬ ì‚¬ìš©

def log_debug(msg):
    with open("debug.log", "a", encoding="utf-8") as f:
        f.write(msg + "\n")

@app.post("/analyze")
@app.post("/api/v1/analyze")
async def analyze_text(request: TextRequest):
    logger.info(f"ğŸ” í…ìŠ¤íŠ¸ ë¶„ì„ ìš”ì²­ ì²˜ë¦¬ ì¤‘... (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(request.text)})")
    with open("debug.log", "a", encoding="utf-8") as f:
        f.write("[DEBUG] analyze_text í•¨ìˆ˜ ì§„ì…!\n")
    log_debug(f"[DEBUG] /analyze ìš”ì²­ ë„ì°©, AI_CLIENT: {AI_CLIENT}")
    log_debug(f"[DEBUG] ì…ë ¥ í…ìŠ¤íŠ¸: {request.text.strip()}")
    try:
        # Gemini 1ìˆœìœ„ ì‹œë„
        if GEMINI_MODEL:
            log_debug("[DEBUG] Gemini AI ë¶„ì„ ë¶„ê¸° ì§„ì… (1ìˆœìœ„)")
            log_debug("[DEBUG] Gemini AI ëª¨ë¸ ìƒíƒœ: ì •ìƒ")
            try:
                prompt = f"""
                MBTI T/F ì„±í–¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ T/F ì„±í–¥ì„ í‰ê°€í•˜ì„¸ìš”.

                [ë¶„ì„ ê¸°ì¤€]
                - T(Thinking): ë…¼ë¦¬ì , ê°ê´€ì , ë¶„ì„ì  ì‚¬ê³ , ì›ì¸ ë¶„ì„, ì²´ê³„ì  ì ‘ê·¼, íš¨ìœ¨ì„± ì¤‘ì‹œ, ë¬¸ì œ í•´ê²° ì§€í–¥
                - F(Feeling): ê°ì •ì , ê³µê°ì , ê´€ê³„ ì¤‘ì‹¬ì  ì‚¬ê³ , ê¸°ë¶„ ê³ ë ¤, ê³µê° í‘œí˜„, ê´€ê³„ ì¤‘ì‹œ, ê°ì •ì  ì§€ì§€
                - ì ìˆ˜: 0(ë§¤ìš° ê°•í•œ T) ~ 100(ë§¤ìš° ê°•í•œ F), 50=ê· í˜•

                [í•µì‹¬ ë¶„ì„ ì›ì¹™]
                1. ë‹µë³€ì˜ ì£¼ìš” ì˜ë„ì™€ í•µì‹¬ ë©”ì‹œì§€ì— ì§‘ì¤‘
                2. T ì„±í–¥ ê°•í•œ í‘œí˜„: "ë¶„ì„", "ì›ì¸", "ë…¼ë¦¬", "ì²´ê³„", "íš¨ìœ¨", "ë°©ì§€", "íŒŒì•…", "ê²°ê³¼", "í•´ê²°", "ì ‘ê·¼", "ë‹¨ê³„ë³„", "ì²´ê³„ì "
                3. F ì„±í–¥ ê°•í•œ í‘œí˜„: "ê¸°ë¶„", "ë§ˆìŒ", "ê³µê°", "í˜ë“¤", "ì•ˆíƒ€ê¹", "ê¶ê¸ˆ", "ë„ì™€", "í•¨ê»˜", "ì§€ì§€", "ìœ„ë¡œ", "ê±±ì •", "ì•ˆíƒ€ê¹"
                4. í˜¼í•© ë‹µë³€ ë¶„ì„: 
                   - T+F í˜¼í•© ë‹µë³€ì˜ ê²½ìš°: í•µì‹¬ ë©”ì‹œì§€ì˜ ë°©í–¥ì„±ì— ë”°ë¼ íŒë‹¨
                   - "ë¶„ì„í•˜ì" + "ìì±…í•˜ì§€ ë§ˆ" â†’ T ì„±í–¥ì´ ìš°ì„  (40-60ì )
                   - "í•¨ê»˜ ìƒê°í•´ë³´ì" â†’ F ì„±í–¥ì´ ìš°ì„  (60-80ì )
                5. ë§¥ë½ë³„ ì ìˆ˜ ê°€ì´ë“œ:
                   - ìˆœìˆ˜ T ì„±í–¥ (ë…¼ë¦¬ì  í•´ê²°): 20-40ì 
                   - T+F í˜¼í•© (ë¶„ì„+ê³µê°): 40-70ì   
                   - ìˆœìˆ˜ F ì„±í–¥ (ê°ì •ì  ì§€ì§€): 70-90ì 

                [ì¶œë ¥ í˜•ì‹]
                [ë¶„ì„] ë‹µë³€ìì˜ T/F ì„±í–¥ ë¶„ì„ (ì„±í–¥ ê°•ë„ì™€ ì£¼ìš” íŠ¹ì§• ëª…ì‹œ)
                [ê·¼ê±°] ë¶„ì„ ê·¼ê±° (êµ¬ì²´ì  í‚¤ì›Œë“œì™€ í‘œí˜„ ë°©ì‹, ì˜ë„ íŒŒì•…)
                [ì œì•ˆ] ê°œì„  ì œì•ˆ 3ê°€ì§€
                [ëŒ€ì•ˆ] ëŒ€ì•ˆ ë‹µë³€
                ì ìˆ˜: X

                ë‹µë³€: {request.text.strip()}
                """
                
                log_debug("[DEBUG] Gemini AIì— ë¶„ì„ ìš”ì²­ ì „ì†¡ ì¤‘...")
                import asyncio
                import re
                response = await asyncio.to_thread(GEMINI_MODEL.generate_content, prompt)
                result = response.text.strip()
                log_debug(f"[Gemini AI ì›ë³¸ ì‘ë‹µ]: {result}")
                log_debug(f"[DEBUG] Gemini AI ì‘ë‹µ ê¸¸ì´: {len(result)} ë¬¸ì")
                
                # ì ìˆ˜ íŒŒì‹± ì •ê·œì‹ ê°œì„ : ë‹¤ì–‘í•œ ë„ì–´ì“°ê¸°/ì½œë¡ /í•œê¸€ì ì˜¤íƒ€ í—ˆìš©
                score_match = re.search(r"ì \s*ìˆ˜\s*[:ï¼š=\-]?\s*(\d{1,3})", result)
                if score_match:
                    tf_score = float(score_match.group(1))
                    log_debug(f"[DEBUG] Gemini AI ì ìˆ˜ íŒŒì‹± ì„±ê³µ: {tf_score}")
                elif (not result) or ("429" in result) or ("QUOTA" in result) or ("ERROR" in result):
                    log_debug("[Gemini ì‘ë‹µ ë¹„ì •ìƒ, Groqë¡œ ì‹œë„]")
                    raise Exception("Gemini ì‘ë‹µ ë¹„ì •ìƒ")
                else:
                    log_debug("[DEBUG] Gemini AI ì ìˆ˜ íŒŒì‹± ì‹¤íŒ¨, í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ ì¶”ì •")
                    if 'T' in result and 'F' not in result:
                        tf_score = 20
                    elif 'F' in result and 'T' not in result:
                        tf_score = 80
                    elif any(k in result for k in ['B', 'ê· í˜•', 'ì¤‘ë¦½', 'ë°¸ëŸ°ìŠ¤']):
                        tf_score = 50
                    elif 'T' in result and 'F' in result:
                        tf_score = 50
                    else:
                        log_debug(f"[Gemini AI ì˜ˆì™¸: ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ] {result}")
                        raise Exception("Gemini ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ")
                    log_debug(f"[DEBUG] í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì • ì ìˆ˜: {tf_score}")
                    log_debug("[ë¶„ì„ ë¡œì§: gemini]")
                
                # ìƒì„¸ë¶„ì„ íŒŒì‹±
                def extract(tag):
                    m = re.search(rf"\[{tag}\](.*?)(?=\[|$)", result, re.DOTALL)
                    return m.group(1).strip() if m else ""
                
                detailed_analysis = extract("ë¶„ì„")
                reasoning = extract("ê·¼ê±°")
                suggestions_raw = extract("ì œì•ˆ")
                suggestions = [s.strip() for s in suggestions_raw.split("\n") if s.strip()] if suggestions_raw else []
                alternative_response = extract("ëŒ€ì•ˆ")
                tip = extract("ì‹¤ì²œíŒ")
                
                log_debug(f"[DEBUG] Gemini AI ë¶„ì„ ê²°ê³¼:")
                log_debug(f"[DEBUG] - ìƒì„¸ë¶„ì„: {detailed_analysis[:100]}...")
                log_debug(f"[DEBUG] - ê·¼ê±°: {reasoning[:100]}...")
                log_debug(f"[DEBUG] - ì œì•ˆ ê°œìˆ˜: {len(suggestions)}")
                log_debug(f"[DEBUG] - ëŒ€ì•ˆ: {alternative_response[:100]}...")
                log_debug(f"[DEBUG] - ì‹¤ì²œíŒ: {tip[:100]}...")
                
                # ëŒ€ì•ˆë‹µë³€ì´ ì—†ê±°ë‚˜ fallbackì¼ ë•Œ ëœë¤ ë¬¸êµ¬ ì¶”ê°€ (Fìš©, Tê°•/ì•½ êµ¬ë¶„)
                if (not alternative_response or alternative_response.strip() == "Gemini ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.") and (not tip or tip.strip() == ""):
                    log_debug("[DEBUG] Gemini AI ëŒ€ì•ˆ ë‹µë³€ ë¶€ì¡±, ëœë¤ ë¬¸êµ¬ ì¶”ê°€")
                    if tf_score <= 20:
                        one_liner = random.choice(get_t_strong_ment())
                    elif tf_score <= 40:
                        one_liner = random.choice(get_t_mild_ment())
                    else:
                        one_liner = random.choice(get_f_friendly_alternatives())
                    # Gemini ëŒ€ì•ˆ ì œì•ˆì´ ìˆìœ¼ë©´ ê·¸ ì•„ë˜ì— ì¶”ê°€
                    gemini_tip = tip
                    gemini_alt = extract("ëŒ€ì•ˆ")
                    merged = []
                    if gemini_tip and gemini_tip.strip() != "Gemini ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
                        merged.append(gemini_tip.strip())
                    if gemini_alt and gemini_alt.strip() != "Gemini ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
                        merged.append(gemini_alt.strip())
                    if merged:
                        alternative_response = one_liner + "\n" + "\n".join(merged)
                else:
                    # ì‹¤ì²œíŒ+ëŒ€ì•ˆì´ ìˆìœ¼ë©´ í•©ì³ì„œ ë°˜í™˜
                    merged = []
                    if tip and tip.strip() != "Gemini ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
                        merged.append(tip.strip())
                    if alternative_response and alternative_response.strip() != "Gemini ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
                        merged.append(alternative_response.strip())
                    if merged:
                        alternative_response = "\n".join(merged)
                
                # --- ìì—°ì–´ ì„±í–¥ íŒŒì‹± ë° ì ìˆ˜ ë³´ì • ---
                def parse_tendency_score(text):
                    text = text.replace(" ", "")
                    # ê°•ë„ ìš°ì„ ìˆœìœ„: ë§¤ìš°ê°•í•œ > ê°•í•œ > ì•½í•œ > ê· í˜•/ì¤‘ë¦½/ë°¸ëŸ°ìŠ¤
                    if re.search(r"ë§¤ìš°ê°•(í•œ)?Tì„±í–¥", text):
                        return 5
                    if re.search(r"ê°•(í•œ)?Tì„±í–¥", text):
                        return 15
                    if re.search(r"ì•½(í•œ)?Tì„±í–¥", text):
                        return 35
                    if re.search(r"Tì™€Fì˜ê· í˜•|ë…¼ë¦¬ì™€ê°ì •ì˜ê· í˜•|ì¤‘ë¦½|ë°¸ëŸ°ìŠ¤", text):
                        return 50
                    if re.search(r"ì•½(í•œ)?Fì„±í–¥", text):
                        return 65
                    if re.search(r"ê°•(í•œ)?Fì„±í–¥", text):
                        return 85
                    if re.search(r"ë§¤ìš°ê°•(í•œ)?Fì„±í–¥", text):
                        return 95
                    if re.search(r"Tì„±í–¥", text):
                        return 40
                    if re.search(r"Fì„±í–¥", text):
                        return 60
                    return None
                
                # ìì—°ì–´ ì„±í–¥ ì ìˆ˜ ì¶”ì¶œ
                tendency_score = parse_tendency_score(detailed_analysis)
                log_debug(f"[DEBUG] ìì—°ì–´ ì„±í–¥ ì ìˆ˜: {tendency_score}")
                # ì ìˆ˜ì™€ ìì—°ì–´ê°€ ë¶ˆì¼ì¹˜í•˜ë©´ ìì—°ì–´ ê¸°ì¤€ìœ¼ë¡œ ë³´ì •
                if tendency_score is not None and abs(tf_score - tendency_score) >= 10:
                    log_debug(f"[ì ìˆ˜/ìì—°ì–´ ë¶ˆì¼ì¹˜: Gemini ì ìˆ˜={tf_score}, ìì—°ì–´ ì ìˆ˜={tendency_score}, ìì—°ì–´ë¡œ ë³´ì •]")
                    tf_score = tendency_score
                
                log_debug(f"[DEBUG] Gemini AI ìµœì¢… ë¶„ì„ ì™„ë£Œ: ì ìˆ˜={tf_score}")
                return AnalysisResponse(
                    score=tf_score,
                    detailed_analysis=detailed_analysis,
                    reasoning=reasoning,
                    suggestions=suggestions,
                    alternative_response=alternative_response
                )
            except Exception as e:
                log_debug(f"[Gemini AI ì˜ˆì™¸ ë°œìƒ, Groqë¡œ ì‹œë„]: {e}")
                # Gemini ì‹¤íŒ¨ ì‹œ Groqë¡œ ì‹œë„
                if AI_CLIENT:
                    try:
                        log_debug("[DEBUG] Groq AI ë¶„ì„ ë¶„ê¸° ì§„ì… (2ìˆœìœ„)")
                        prompt = f"""
                        ì•„ë˜ ë‹µë³€ì€ T(ì‚¬ê³ í˜•)ì¸ ë‚´ê°€ F(ê°ì •í˜•)ì¸ ìƒëŒ€ì—ê²Œ í•œ ë§ì´ì•¼.
                        - F(ê°ì •í˜•) ì„±í–¥ì˜ ìƒëŒ€ê°€ ì´ ë‹µë³€ì„ ë“¤ì—ˆì„ ë•Œ ì–´ë–¤ ëŠë‚Œì¼ì§€, ê·¸ë¦¬ê³  Fì—ê²Œ ë” íš¨ê³¼ì ìœ¼ë¡œ ì†Œí†µí•˜ë ¤ë©´ ì–´ë–»ê²Œ ë°”ê¾¸ë©´ ì¢‹ì„ì§€ ë¶„ì„í•´ì¤˜.
                        - ë¶„ì„ ê²°ê³¼(ìì—°ì–´)ì—ëŠ” ë°˜ë“œì‹œ 'ë§¤ìš° ê°•í•œ T ì„±í–¥', 'ê°•í•œ F ì„±í–¥', 'ì•½í•œ T ì„±í–¥', 'Tì™€ Fì˜ ê· í˜•', 'ì¤‘ë¦½', 'ë°¸ëŸ°ìŠ¤' ë“±ê³¼ ê°™ì´ 'ì„±í–¥ì´ OOOí•˜ë‹¤'ë¼ëŠ” ë¬¸êµ¬ë¥¼ ëª…í™•í•˜ê²Œ í¬í•¨í•´ì„œ ì‘ì„±í•´ì¤˜.
                        - ë§ˆì§€ë§‰ì—ëŠ” ë°˜ë“œì‹œ "ì ìˆ˜: X" í˜•ì‹ìœ¼ë¡œ 0~100 ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ëª…ì‹œí•´ì¤˜. (0=ë§¤ìš° ê°•í•œ T, 50=ê· í˜•, 100=ë§¤ìš° ê°•í•œ F)
                        - ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜:
                        [ë¶„ì„]
                        ì„±í–¥ ë¶„ì„ ë° F ì…ì¥ì—ì„œì˜ ë°˜ì‘

                        [ê·¼ê±°]
                        ë¶„ì„ì˜ ê·¼ê±°

                        [ì œì•ˆ]
                        1. Fê°€ ê³µê°í•  ìˆ˜ ìˆëŠ” ê°œì„  ì œì•ˆ 1
                        2. Fê°€ ê³µê°í•  ìˆ˜ ìˆëŠ” ê°œì„  ì œì•ˆ 2
                        3. Fê°€ ê³µê°í•  ìˆ˜ ìˆëŠ” ê°œì„  ì œì•ˆ 3

                        [ì‹¤ì²œíŒ]
                        F ì„±í–¥ ìƒëŒ€ë¥¼ ìœ„í•œ í•œ ì¤„ ì‹¤ì²œ íŒ

                        [ëŒ€ì•ˆ]
                        F ì„±í–¥ ìƒëŒ€ë¥¼ ìœ„í•œ ëŒ€ì•ˆ ë‹µë³€

                        ì ìˆ˜: X (0=ë§¤ìš° ê°•í•œ T, 50=ê· í˜•, 100=ë§¤ìš° ê°•í•œ F)

                        *** ì¤‘ìš”: ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì˜ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ***

                        ë‹µë³€: {request.text.strip()}
                        """
                        response = await AI_CLIENT.chat.completions.create(
                            messages=[{"role": "user", "content": prompt}],
                            model="llama3-8b-8192",
                        )
                        result = response.choices[0].message.content
                        if result is not None:
                            result = result.strip().upper()
                        else:
                            result = ""
                        log_debug(f"[Groq AI ì›ë³¸ ì‘ë‹µ]: {result}")
                        
                        # ì ìˆ˜ íŒŒì‹± ì •ê·œì‹ ê°œì„ : ë‹¤ì–‘í•œ ë„ì–´ì“°ê¸°/ì½œë¡ /í•œê¸€ì ì˜¤íƒ€ í—ˆìš©
                        score_match = re.search(r"ì \s*ìˆ˜\s*[:ï¼š=\-]?\s*(\d{1,3})", result)
                        if score_match:
                            tf_score = float(score_match.group(1))
                        elif (not result) or ("429" in result) or ("QUOTA" in result) or ("ERROR" in result):
                            log_debug("[Groq ì‘ë‹µ ë¹„ì •ìƒ, fallbackìœ¼ë¡œ ìì²´ ë¶„ì„ ìˆ˜í–‰]")
                            tf_score = analyze_tf_tendency(request.text)
                            log_debug("[ë¶„ì„ ë¡œì§: fallback]")
                        else:
                            if 'T' in result and 'F' not in result:
                                tf_score = 20
                            elif 'F' in result and 'T' not in result:
                                tf_score = 80
                            elif any(k in result for k in ['B', 'ê· í˜•', 'ì¤‘ë¦½', 'ë°¸ëŸ°ìŠ¤']):
                                tf_score = 50
                            elif 'T' in result and 'F' in result:
                                tf_score = 50
                            else:
                                log_debug(f"[Groq AI ì˜ˆì™¸: ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ] {result}")
                                tf_score = analyze_tf_tendency(request.text)
                                log_debug("[ë¶„ì„ ë¡œì§: fallback]")
                            log_debug("[ë¶„ì„ ë¡œì§: groq]")
                        
                        # ìƒì„¸ë¶„ì„ íŒŒì‹±
                        def extract(tag):
                            content = response.choices[0].message.content
                            if content is None:
                                return ""
                            m = re.search(rf"\[{tag}\](.*?)(?=\[|$)", content, re.DOTALL)
                            return m.group(1).strip() if m else ""
                        
                        detailed_analysis = extract("ë¶„ì„")
                        reasoning = extract("ê·¼ê±°")
                        suggestions_raw = extract("ì œì•ˆ")
                        suggestions = [s.strip() for s in suggestions_raw.split("\n") if s.strip()] if suggestions_raw else []
                        alternative_response = extract("ëŒ€ì•ˆ")
                        tip = extract("ì‹¤ì²œíŒ")
                        
                        # ëŒ€ì•ˆë‹µë³€ì´ ì—†ê±°ë‚˜ fallbackì¼ ë•Œ ëœë¤ ë¬¸êµ¬ ì¶”ê°€ (Fìš©, Tê°•/ì•½ êµ¬ë¶„)
                        if (not alternative_response or alternative_response.strip() == "Groq ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.") and (not tip or tip.strip() == ""):
                            if tf_score <= 20:
                                one_liner = random.choice(get_t_strong_ment())
                            elif tf_score <= 40:
                                one_liner = random.choice(get_t_mild_ment())
                            else:
                                one_liner = random.choice(get_f_friendly_alternatives())
                            # Groq ëŒ€ì•ˆ ì œì•ˆì´ ìˆìœ¼ë©´ ê·¸ ì•„ë˜ì— ì¶”ê°€
                            groq_tip = tip
                            groq_alt = extract("ëŒ€ì•ˆ")
                            merged = []
                            if groq_tip and groq_tip.strip() != "Groq ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
                                merged.append(groq_tip.strip())
                            if groq_alt and groq_alt.strip() != "Groq ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
                                merged.append(groq_alt.strip())
                            if merged:
                                alternative_response = one_liner + "\n" + "\n".join(merged)
                            else:
                                alternative_response = one_liner
                        else:
                            # ì‹¤ì²œíŒ+ëŒ€ì•ˆì´ ìˆìœ¼ë©´ í•©ì³ì„œ ë°˜í™˜
                            merged = []
                            if tip and tip.strip() != "Groq ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
                                merged.append(tip.strip())
                            if alternative_response and alternative_response.strip() != "Groq ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
                                merged.append(alternative_response.strip())
                            if merged:
                                alternative_response = "\n".join(merged)
                        
                        # --- ìì—°ì–´ ì„±í–¥ íŒŒì‹± ë° ì ìˆ˜ ë³´ì • ---
                        def parse_tendency_score(text):
                            text = text.replace(" ", "")
                            # ê°•ë„ ìš°ì„ ìˆœìœ„: ë§¤ìš°ê°•í•œ > ê°•í•œ > ì•½í•œ > ê· í˜•/ì¤‘ë¦½/ë°¸ëŸ°ìŠ¤
                            if re.search(r"ë§¤ìš°ê°•(í•œ)?Tì„±í–¥", text):
                                return 5
                            if re.search(r"ê°•(í•œ)?Tì„±í–¥", text):
                                return 15
                            if re.search(r"ì•½(í•œ)?Tì„±í–¥", text):
                                return 35
                            if re.search(r"Tì™€Fì˜ê· í˜•|ë…¼ë¦¬ì™€ê°ì •ì˜ê· í˜•|ì¤‘ë¦½|ë°¸ëŸ°ìŠ¤", text):
                                return 50
                            if re.search(r"ì•½(í•œ)?Fì„±í–¥", text):
                                return 65
                            if re.search(r"ê°•(í•œ)?Fì„±í–¥", text):
                                return 85
                            if re.search(r"ë§¤ìš°ê°•(í•œ)?Fì„±í–¥", text):
                                return 95
                            if re.search(r"Tì„±í–¥", text):
                                return 40
                            if re.search(r"Fì„±í–¥", text):
                                return 60
                            return None
                        
                        # ìì—°ì–´ ì„±í–¥ ì ìˆ˜ ì¶”ì¶œ
                        tendency_score = parse_tendency_score(detailed_analysis)
                        # ì ìˆ˜ì™€ ìì—°ì–´ê°€ ë¶ˆì¼ì¹˜í•˜ë©´ ìì—°ì–´ ê¸°ì¤€ìœ¼ë¡œ ë³´ì •
                        if tendency_score is not None and abs(tf_score - tendency_score) >= 10:
                            log_debug(f"[ì ìˆ˜/ìì—°ì–´ ë¶ˆì¼ì¹˜: Groq ì ìˆ˜={tf_score}, ìì—°ì–´ ì ìˆ˜={tendency_score}, ìì—°ì–´ë¡œ ë³´ì •]")
                            tf_score = tendency_score
                        
                        return AnalysisResponse(
                            score=tf_score,
                            detailed_analysis=detailed_analysis,
                            reasoning=reasoning,
                            suggestions=suggestions,
                            alternative_response=alternative_response
                        )
                    except Exception as groq_e:
                        log_debug(f"[Groq AI ì˜ˆì™¸ ë°œìƒ, fallbackìœ¼ë¡œ ìì²´ ë¶„ì„ ìˆ˜í–‰]: {groq_e}")
                        tf_score = analyze_tf_tendency(request.text)
                        log_debug("[ë¶„ì„ ë¡œì§: fallback]")
                        return AnalysisResponse(score=tf_score)
                else:
                    # Groqë„ ì—†ìœ¼ë©´ fallback
                    tf_score = analyze_tf_tendency(request.text)
                    log_debug("[ë¶„ì„ ë¡œì§: fallback]")
                    return AnalysisResponse(score=tf_score)
        # Geminiê°€ ì—†ìœ¼ë©´ Groq ì‹œë„
        elif AI_CLIENT:
            log_debug("[DEBUG] Groq AI ë¶„ì„ ë¶„ê¸° ì§„ì… (1ìˆœìœ„)")
            try:
                prompt = f"""
                ì•„ë˜ ë‹µë³€ì€ T(ì‚¬ê³ í˜•)ì¸ ë‚´ê°€ F(ê°ì •í˜•)ì¸ ìƒëŒ€ì—ê²Œ í•œ ë§ì´ì•¼.
                - F(ê°ì •í˜•) ì„±í–¥ì˜ ìƒëŒ€ê°€ ì´ ë‹µë³€ì„ ë“¤ì—ˆì„ ë•Œ ì–´ë–¤ ëŠë‚Œì¼ì§€, ê·¸ë¦¬ê³  Fì—ê²Œ ë” íš¨ê³¼ì ìœ¼ë¡œ ì†Œí†µí•˜ë ¤ë©´ ì–´ë–»ê²Œ ë°”ê¾¸ë©´ ì¢‹ì„ì§€ ë¶„ì„í•´ì¤˜.
                - ë¶„ì„ ê²°ê³¼(ìì—°ì–´)ì—ëŠ” ë°˜ë“œì‹œ 'ë§¤ìš° ê°•í•œ T ì„±í–¥', 'ê°•í•œ F ì„±í–¥', 'ì•½í•œ T ì„±í–¥', 'Tì™€ Fì˜ ê· í˜•', 'ì¤‘ë¦½', 'ë°¸ëŸ°ìŠ¤' ë“±ê³¼ ê°™ì´ 'ì„±í–¥ì´ OOOí•˜ë‹¤'ë¼ëŠ” ë¬¸êµ¬ë¥¼ ëª…í™•í•˜ê²Œ í¬í•¨í•´ì„œ ì‘ì„±í•´ì¤˜.
                - ë§ˆì§€ë§‰ì—ëŠ” ë°˜ë“œì‹œ "ì ìˆ˜: X" í˜•ì‹ìœ¼ë¡œ 0~100 ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ëª…ì‹œí•´ì¤˜. (0=ë§¤ìš° ê°•í•œ T, 50=ê· í˜•, 100=ë§¤ìš° ê°•í•œ F)
                - ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜:
                [ë¶„ì„]
                ì„±í–¥ ë¶„ì„ ë° F ì…ì¥ì—ì„œì˜ ë°˜ì‘

                [ê·¼ê±°]
                ë¶„ì„ì˜ ê·¼ê±°

                [ì œì•ˆ]
                1. Fê°€ ê³µê°í•  ìˆ˜ ìˆëŠ” ê°œì„  ì œì•ˆ 1
                2. Fê°€ ê³µê°í•  ìˆ˜ ìˆëŠ” ê°œì„  ì œì•ˆ 2
                3. Fê°€ ê³µê°í•  ìˆ˜ ìˆëŠ” ê°œì„  ì œì•ˆ 3

                [ì‹¤ì²œíŒ]
                F ì„±í–¥ ìƒëŒ€ë¥¼ ìœ„í•œ í•œ ì¤„ ì‹¤ì²œ íŒ

                [ëŒ€ì•ˆ]
                F ì„±í–¥ ìƒëŒ€ë¥¼ ìœ„í•œ ëŒ€ì•ˆ ë‹µë³€

                ì ìˆ˜: X (0=ë§¤ìš° ê°•í•œ T, 50=ê· í˜•, 100=ë§¤ìš° ê°•í•œ F)

                *** ì¤‘ìš”: ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì˜ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ***

                ë‹µë³€: {request.text.strip()}
                """
                response = await AI_CLIENT.chat.completions.create(
                    messages=[{"role": "user", "content": prompt}],
                    model="llama3-8b-8192",
                )
                result = response.choices[0].message.content
                if result is not None:
                    result = result.strip().upper()
                else:
                    result = ""
                log_debug(f"[Groq AI ì›ë³¸ ì‘ë‹µ]: {result}")
                import re
                # ì ìˆ˜ íŒŒì‹± ì •ê·œì‹ ê°œì„ : ë‹¤ì–‘í•œ ë„ì–´ì“°ê¸°/ì½œë¡ /í•œê¸€ì ì˜¤íƒ€ í—ˆìš©
                score_match = re.search(r"ì \s*ìˆ˜\s*[:ï¼š=\-]?\s*(\d{1,3})", result)
                if score_match:
                    tf_score = float(score_match.group(1))
                elif (not result) or ("429" in result) or ("QUOTA" in result) or ("ERROR" in result):
                    log_debug("[Groq ì‘ë‹µ ë¹„ì •ìƒ, fallbackìœ¼ë¡œ ìì²´ ë¶„ì„ ìˆ˜í–‰]")
                    tf_score = analyze_tf_tendency(request.text)
                    log_debug("[ë¶„ì„ ë¡œì§: fallback]")
                else:
                    if 'T' in result and 'F' not in result:
                        tf_score = 20
                    elif 'F' in result and 'T' not in result:
                        tf_score = 80
                    elif any(k in result for k in ['B', 'ê· í˜•', 'ì¤‘ë¦½', 'ë°¸ëŸ°ìŠ¤']):
                        tf_score = 50
                    elif 'T' in result and 'F' in result:
                        tf_score = 50
                    else:
                        log_debug(f"[Groq AI ì˜ˆì™¸: ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ] {result}")
                        tf_score = analyze_tf_tendency(request.text)
                        log_debug("[ë¶„ì„ ë¡œì§: fallback]")
                    log_debug("[ë¶„ì„ ë¡œì§: groq]")
                # ìƒì„¸ë¶„ì„ íŒŒì‹±
                def extract(tag):
                    content = response.choices[0].message.content
                    if content is None:
                        return ""
                    m = re.search(rf"\[{tag}\](.*?)(?=\[|$)", content, re.DOTALL)
                    return m.group(1).strip() if m else ""
                detailed_analysis = extract("ë¶„ì„")
                reasoning = extract("ê·¼ê±°")
                suggestions_raw = extract("ì œì•ˆ")
                suggestions = [s.strip() for s in suggestions_raw.split("\n") if s.strip()] if suggestions_raw else []
                alternative_response = extract("ëŒ€ì•ˆ")
                tip = extract("ì‹¤ì²œíŒ")
                # ëŒ€ì•ˆë‹µë³€ì´ ì—†ê±°ë‚˜ fallbackì¼ ë•Œ ëœë¤ ë¬¸êµ¬ ì¶”ê°€ (Fìš©, Tê°•/ì•½ êµ¬ë¶„)
                if (not alternative_response or alternative_response.strip() == "Groq ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.") and (not tip or tip.strip() == ""):
                    if tf_score <= 20:
                        one_liner = random.choice(get_t_strong_ment())
                    elif tf_score <= 40:
                        one_liner = random.choice(get_t_mild_ment())
                    else:
                        one_liner = random.choice(get_f_friendly_alternatives())
                    # Groq ëŒ€ì•ˆ ì œì•ˆì´ ìˆìœ¼ë©´ ê·¸ ì•„ë˜ì— ì¶”ê°€
                    groq_tip = tip
                    groq_alt = extract("ëŒ€ì•ˆ")
                    merged = []
                    if groq_tip and groq_tip.strip() != "Groq ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
                        merged.append(groq_tip.strip())
                    if groq_alt and groq_alt.strip() != "Groq ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
                        merged.append(groq_alt.strip())
                    if merged:
                        alternative_response = one_liner + "\n" + "\n".join(merged)
                    else:
                        alternative_response = one_liner
                else:
                    # ì‹¤ì²œíŒ+ëŒ€ì•ˆì´ ìˆìœ¼ë©´ í•©ì³ì„œ ë°˜í™˜
                    merged = []
                    if tip and tip.strip() != "Groq ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
                        merged.append(tip.strip())
                    if alternative_response and alternative_response.strip() != "Groq ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
                        merged.append(alternative_response.strip())
                    if merged:
                        alternative_response = "\n".join(merged)
                # --- ìì—°ì–´ ì„±í–¥ íŒŒì‹± ë° ì ìˆ˜ ë³´ì • ---
                def parse_tendency_score(text):
                    text = text.replace(" ", "")
                    # ê°•ë„ ìš°ì„ ìˆœìœ„: ë§¤ìš°ê°•í•œ > ê°•í•œ > ì•½í•œ > ê· í˜•/ì¤‘ë¦½/ë°¸ëŸ°ìŠ¤
                    if re.search(r"ë§¤ìš°ê°•(í•œ)?Tì„±í–¥", text):
                        return 5
                    if re.search(r"ê°•(í•œ)?Tì„±í–¥", text):
                        return 15
                    if re.search(r"ì•½(í•œ)?Tì„±í–¥", text):
                        return 35
                    if re.search(r"Tì™€Fì˜ê· í˜•|ë…¼ë¦¬ì™€ê°ì •ì˜ê· í˜•|ì¤‘ë¦½|ë°¸ëŸ°ìŠ¤", text):
                        return 50
                    if re.search(r"ì•½(í•œ)?Fì„±í–¥", text):
                        return 65
                    if re.search(r"ê°•(í•œ)?Fì„±í–¥", text):
                        return 85
                    if re.search(r"ë§¤ìš°ê°•(í•œ)?Fì„±í–¥", text):
                        return 95
                    if re.search(r"Tì„±í–¥", text):
                        return 40
                    if re.search(r"Fì„±í–¥", text):
                        return 60
                    return None
                # ìì—°ì–´ ì„±í–¥ ì ìˆ˜ ì¶”ì¶œ
                tendency_score = parse_tendency_score(detailed_analysis)
                # ì ìˆ˜ì™€ ìì—°ì–´ê°€ ë¶ˆì¼ì¹˜í•˜ë©´ ìì—°ì–´ ê¸°ì¤€ìœ¼ë¡œ ë³´ì •
                if tendency_score is not None and abs(tf_score - tendency_score) >= 10:
                    log_debug(f"[ì ìˆ˜/ìì—°ì–´ ë¶ˆì¼ì¹˜: Groq ì ìˆ˜={tf_score}, ìì—°ì–´ ì ìˆ˜={tendency_score}, ìì—°ì–´ë¡œ ë³´ì •]")
                    tf_score = tendency_score
                return AnalysisResponse(
                    score=tf_score,
                    detailed_analysis=detailed_analysis,
                    reasoning=reasoning,
                    suggestions=suggestions,
                    alternative_response=alternative_response
                )
            except Exception as e:
                log_debug(f"[Groq AI ì˜ˆì™¸ ë°œìƒ, fallbackìœ¼ë¡œ ìì²´ ë¶„ì„ ìˆ˜í–‰]: {e}")
                tf_score = analyze_tf_tendency(request.text)
                log_debug("[ë¶„ì„ ë¡œì§: fallback]")
                return AnalysisResponse(score=tf_score)
        else:
            log_debug("[DEBUG] Fallback(í‚¤ì›Œë“œ ë¶„ì„) ë¶„ê¸° ì§„ì…")
            tf_score = analyze_tf_tendency(request.text)
            log_debug("[ë¶„ì„ ë¡œì§: fallback]")
            return AnalysisResponse(score=tf_score)
    except Exception as e:
        log_debug(f"[analyze_text ìµœìƒìœ„ ì˜ˆì™¸]: {e}")
        tf_score = analyze_tf_tendency(request.text)
        log_debug("[ë¶„ì„ ë¡œì§: fallback]")
        return AnalysisResponse(score=tf_score)

@app.post("/final_analyze")
@app.post("/api/v1/final_analyze")
async def final_analyze(request: FinalAnalysisRequest):
    logger.info(f"ğŸ” ìµœì¢… ë¶„ì„ ìš”ì²­ ì²˜ë¦¬ ì¤‘... (ê²°ê³¼ ê°œìˆ˜: {len(request.results)})")
    try:
        final_result = generate_final_analysis(request.results)
        logger.info("âœ… ìµœì¢… ë¶„ì„ ì™„ë£Œ")
        return final_result
    except Exception as e:
        logger.error(f"âŒ ìµœì¢… ë¶„ì„ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/generate_questions")
@app.post("/api/v1/generate_questions")
async def generate_questions(request: QuestionGenerationRequest):
    """
    AI ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ì§ˆë¬¸ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        questions = await generate_ai_questions_real(count=request.count or 5, difficulty=request.difficulty or "medium")
        return {
            "questions": questions,
            "count": len(questions),
            "difficulty": request.difficulty,
            "generated_by": "AI"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.get("/questions")
@app.get("/api/v1/questions")
async def get_questions(count: int = 5):
    """
    questions.json íŒŒì¼ì—ì„œ ì§ˆë¬¸ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    logger.info(f"ğŸ” ì§ˆë¬¸ ë¡œë“œ ìš”ì²­ ì²˜ë¦¬ ì¤‘... (count: {count})")
    try:
        # questions.json íŒŒì¼ ì‚¬ìš©
        questions_file = Path("question/questions.json")
        if not questions_file.exists():
            raise HTTPException(status_code=404, detail="ì§ˆë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        with open(questions_file, "r", encoding="utf-8") as f:
            questions_data = json.load(f)
        
        questions_list = questions_data.get("questions", [])
        
        # ìš”ì²­ëœ ê°œìˆ˜ë§Œí¼ ëœë¤ ì„ íƒ (Fisher-Yates ì…”í”Œ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
        import random
        if len(questions_list) > count:
            # ì „ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…”í”Œí•œ í›„ ì•ì—ì„œë¶€í„° ì„ íƒ
            shuffled = questions_list.copy()
            random.shuffle(shuffled)
            selected_questions = shuffled[:count]
        else:
            # ì „ì²´ ë¦¬ìŠ¤íŠ¸ê°€ ìš”ì²­ ê°œìˆ˜ë³´ë‹¤ ì ìœ¼ë©´ ì „ì²´ë¥¼ ì…”í”Œí•´ì„œ ë°˜í™˜
            selected_questions = questions_list.copy()
            random.shuffle(selected_questions)
        
        return {
            "questions": selected_questions,
            "source": "questions.json",
            "count": len(selected_questions),
            "total_available": len(questions_list)
        }
            
    except json.JSONDecodeError:
        # JSON íŒŒì¼ ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ì§ˆë¬¸ ì‚¬ìš©
        return {
            "questions": ["ì§ˆë¬¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."],
            "source": "error_fallback",
            "count": 1,
            "difficulty": "medium"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reset_log")
async def reset_log():
    try:
        with open("debug.log", "w", encoding="utf-8") as f:
            f.write("[DEBUG] ë¡œê·¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!\n")
            f.write(f"[DEBUG] ì´ˆê¸°í™” ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("[DEBUG] UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ ì •ìƒ ì´ˆê¸°í™”ë¨\n")
        logger.info("âœ… ë¡œê·¸ íŒŒì¼ ì´ˆê¸°í™” ì™„ë£Œ")
        return Response(content="ë¡œê·¸ ì´ˆê¸°í™” ì™„ë£Œ", media_type="text/plain")
    except Exception as e:
        logger.error(f"âŒ ë¡œê·¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë¡œê·¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")

def get_t_strong_ment():
    return [
        "ë¼ˆ ë§ì•˜ì–´â€¦",
        "ì–´ë””ë³´ì ë°ìŠ¤ë…¸íŠ¸ê°€...",
        "ë³µìˆ˜í•œë‹¤â€¦",
        "ë„Œ ì§„ì§œ ê°ì •ì´ë€ ê²Œ ìˆë‹ˆ?",
        "ë„Œ Dì¡Œë‹¤",
        "ì¡°ë§Œê°„ ìˆœì‚´ë§Œë“¤ì–´ì¤€ë‹¤",
        "ë¡œë´‡ì´ëƒ..?",
        "ìœ  ìŠ¤í‹¸ ë§ˆì´ ë°ìŠ¤ë…¸íŠ¸ ë„˜ë²„ì›~",
        "ìš°ë¦¬ í—¤ì–´ì ¸",
        "ì €ë¦¬ê°€ ã… ã… "
    ]

def get_t_mild_ment():
    return [
        "ê³„ì‚°ê¸°ëƒ?",
        "ë‹˜ ë°°ë ¤ì¢€...",
        "ë¡œë´‡ì´ëƒ?",
        "ì‚´ì‚´í•´ì£¼ì„¸ìš”..",
        "ë‹ˆ ë§ë„ ë§ëŠ”ë°.. ì‚´ì‚´ì¢€ ã… ",
        "ë‚´ ê¸°ë¶„ ì¡´ì¤‘ì¢€ ã… ",
        "ë§ ëŒ€ì‹  ê²°ê³¼?",
        "ê°ì •ë„ ì¢€ ì±™ê¸°ë¼êµ¬!",
        "ë„¤ ë…¼ë¦¬ ë”°ë¼ê°€ë‹¤ ë¨¸ë¦¬ í„°ì ¸ ì£½ê² ì–´",
        "íŒ©íŠ¸ë¶€í„° ì •ë¦¬í•´ë¼? ë‚´ ë§ˆìŒì€ ëˆ„ê°€ ì •ë¦¬í•´ì¤˜?"
    ]

@app.post("/stt")
@app.post("/api/v1/stt")
async def speech_to_text(audio_file: UploadFile = File(...)):
    """
    Speech to text endpoint that accepts audio file uploads
    """
    logger.info(f"ğŸ” STT ìš”ì²­ ì²˜ë¦¬ ì¤‘... (íŒŒì¼ëª…: {audio_file.filename})")
    try:
        if not audio_file:
            logger.error("âŒ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì œê³µë˜ì§€ ì•ŠìŒ")
            raise HTTPException(status_code=400, detail="No audio file provided")
        
        # íŒŒì¼ í™•ì¥ì ê²€ì‚¬
        allowed_extensions = ['.wav', '.mp3', '.ogg', '.m4a']
        filename = audio_file.filename or ""
        file_ext = os.path.splitext(filename)[1].lower()
        if file_ext not in allowed_extensions:
            raise HTTPException(
                status_code=400,
                detail=f"Unsupported file format. Allowed formats: {', '.join(allowed_extensions)}"
            )
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        temp_audio_path = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as temp_audio:
                temp_audio_path = temp_audio.name
                content = await audio_file.read()
                temp_audio.write(content)
                temp_audio.flush()
                os.fsync(temp_audio.fileno())
            
            print(f"Processing audio file: {temp_audio_path}")
            # ëª¨ë“ˆí™”ëœ STT í•¨ìˆ˜ ì‚¬ìš©
            text = transcribe_audio_file(temp_audio_path)
            print(f"STT result: {text}")
            return {"text": text}
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if temp_audio_path and os.path.exists(temp_audio_path):
                try:
                    os.unlink(temp_audio_path)
                except Exception as e:
                    print(f"Failed to delete temp file: {e}")
    except Exception as e:
        print(f"STT Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/stt_enhanced")
@app.post("/api/v1/stt_enhanced")
async def speech_to_text_enhanced(audio_file: UploadFile = File(...)):
    """
    í–¥ìƒëœ STT ê¸°ëŠ¥ - ë” ì •í™•í•œ ìŒì„± ì¸ì‹ê³¼ í’ˆì§ˆ ê²€ì¦
    """
    logger.info(f"ğŸ” í–¥ìƒëœ STT ìš”ì²­ ì²˜ë¦¬ ì¤‘... (íŒŒì¼ëª…: {audio_file.filename})")
    
    try:
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_file:
            content = await audio_file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        print(f"Processing audio file: {temp_file_path}")
        
        try:
            # 1ë‹¨ê³„: ì˜¤ë””ì˜¤ í’ˆì§ˆ ê²€ì¦
            quality_result = validate_audio_quality(temp_file_path)
            
            # numpy íƒ€ì…ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            if isinstance(quality_result, dict):
                for key, value in quality_result.items():
                    if hasattr(value, 'item'):  # numpy íƒ€ì…ì¸ ê²½ìš°
                        quality_result[key] = float(value)
                    elif isinstance(value, (list, dict)):
                        # ë¦¬ìŠ¤íŠ¸ë‚˜ ë”•ì…”ë„ˆë¦¬ ë‚´ë¶€ì˜ numpy íƒ€ì…ë„ ë³€í™˜
                        if isinstance(value, list):
                            quality_result[key] = [float(v) if hasattr(v, 'item') else v for v in value]
            
            # 2ë‹¨ê³„: í–¥ìƒëœ STT ì²˜ë¦¬
            stt_result = transcribe_audio_file_enhanced(temp_file_path)
            
            # 3ë‹¨ê³„: ê²°ê³¼ ì •ë¦¬
            response_data = {
                "text": stt_result["text"],
                "original_text": stt_result["original_text"],
                "confidence": stt_result["confidence"],
                "alternatives": stt_result["alternatives"],
                "has_alternatives": stt_result["has_alternatives"],
                "audio_quality": quality_result,
                "suggestions": []
            }
            
            # 4ë‹¨ê³„: í’ˆì§ˆ ê¸°ë°˜ ì œì•ˆ ì¶”ê°€
            if not quality_result["is_good"]:
                response_data["suggestions"].extend(quality_result["suggestions"])
            
            if stt_result["confidence"] < 0.7:
                response_data["suggestions"].append("ìŒì„± ì¸ì‹ ì •í™•ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ë” ëª…í™•í•˜ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”.")
            
            logger.info(f"í–¥ìƒëœ STT ê²°ê³¼: '{stt_result['text']}' (ì‹ ë¢°ë„: {stt_result['confidence']:.2f})")
            
            return response_data
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                
    except Exception as e:
        logger.error(f"í–¥ìƒëœ STT ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"í–¥ìƒëœ STT ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/correct_sentence")
@app.post("/api/v1/correct_sentence")
async def correct_sentence(request: SentenceCorrectionRequest):
    """
    STT ê²°ê³¼ ë¬¸ì¥ì„ êµì •í•©ë‹ˆë‹¤.
    """
    logger.info(f"ğŸ” ë¬¸ì¥ êµì • ìš”ì²­ ì²˜ë¦¬ ì¤‘... (í…ìŠ¤íŠ¸: {request.text})")
    
    try:
        # ë¬¸ì¥ êµì • í”„ë¡¬í”„íŠ¸ ìƒì„± (ë” êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§€ì‹œì‚¬í•­)
        prompt = f"""
ë‹¤ìŒ ìŒì„± ì¸ì‹ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ½ê³  ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ êµì •í•´ì£¼ì„¸ìš”.

êµì • ê·œì¹™:
1. ì˜¤íƒ€ë‚˜ ì˜ëª»ëœ ë‹¨ì–´ë¥¼ ì˜¬ë°”ë¥¸ ë‹¨ì–´ë¡œ ìˆ˜ì •
2. ë¬¸ë²• ì˜¤ë¥˜ë¥¼ ìˆ˜ì • (ì¡°ì‚¬, ì–´ë¯¸ ë“±)
3. ë¶ˆì™„ì „í•œ ë¬¸ì¥ì„ ì™„ì„±
4. ì›ë˜ ì˜ë¯¸ëŠ” ë°˜ë“œì‹œ ìœ ì§€
5. êµì •ëœ ë¬¸ì¥ë§Œ ì¶œë ¥ (ì„¤ëª… ì—†ì´)

ìŒì„± ì¸ì‹ ê²°ê³¼: "{request.text}"

êµì •ëœ ë¬¸ì¥:
"""
        
        # AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ êµì •
        try:
            # Gemini AI ì§ì ‘ ì‚¬ìš©
            import google.generativeai as genai
            from mbti_analyzer.config.settings import settings
            
            # Gemini AI ì„¤ì •
            import os
            gemini_key = os.getenv('GEMINI_API_KEY')
            if not gemini_key:
                # settingsì—ì„œ í™•ì¸
                gemini_key = settings.gemini_api_key
                if not gemini_key:
                    raise Exception("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ GEMINI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            
            logger.info(f"Gemini API í‚¤ í™•ì¸: {gemini_key[:10]}...")
            genai.configure(api_key=gemini_key)
            
            model = genai.GenerativeModel('gemini-1.5-flash')
            
            # AI ì‘ë‹µ ìƒì„±
            response = model.generate_content(prompt)
            corrected_text = response.text.strip()
            
            logger.info(f"AI ì‘ë‹µ ì›ë³¸: {corrected_text}")
            
            # êµì • ê²°ê³¼ ì •ë¦¬
            if corrected_text and isinstance(corrected_text, str):
                # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
                corrected_text = corrected_text.replace("êµì •ëœ ë¬¸ì¥:", "").strip()
                corrected_text = corrected_text.replace("êµì •:", "").strip()
                corrected_text = corrected_text.replace("ìˆ˜ì •ëœ ë¬¸ì¥:", "").strip()
                corrected_text = corrected_text.replace("ê²°ê³¼:", "").strip()
                corrected_text = corrected_text.replace("ë‹µë³€:", "").strip()
                
                # ì¤„ë°”ê¿ˆ ì •ë¦¬
                corrected_text = corrected_text.replace("\n", " ").strip()
                
                # ë”°ì˜´í‘œ ì œê±°
                corrected_text = corrected_text.strip('"').strip("'").strip()
                
                # ë¹ˆ ë¬¸ìì—´ ì²´í¬
                if not corrected_text:
                    corrected_text = request.text
                
                logger.info(f"ì •ë¦¬ëœ êµì • ê²°ê³¼: '{corrected_text}'")
                
                # ì›ë³¸ê³¼ ë™ì¼í•œ ê²½ìš° ì›ë³¸ ë°˜í™˜
                has_changes = corrected_text != request.text
                
                if not has_changes:
                    logger.info("êµì • ê²°ê³¼ê°€ ì›ë³¸ê³¼ ë™ì¼í•¨")
                
                return {
                    "success": True,
                    "original_text": request.text,
                    "corrected_text": corrected_text,
                    "has_changes": has_changes,
                    "ai_response": response.text.strip()  # ë””ë²„ê¹…ìš©
                }
            else:
                raise Exception("AI ë¬¸ì¥ êµì • ìƒì„± ì‹¤íŒ¨")
                
        except Exception as ai_error:
            logger.error(f"AI ë¬¸ì¥ êµì • ì‹¤íŒ¨: {ai_error}")
            # AI ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
            return {
                "success": True,
                "original_text": request.text,
                "corrected_text": request.text,
                "has_changes": False,
                "fallback": True,
                "error": str(ai_error),
                "error_type": type(ai_error).__name__
            }
            
    except Exception as e:
        logger.error(f"ë¬¸ì¥ êµì • ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë¬¸ì¥ êµì • ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/correct_sentence_enhanced")
@app.post("/api/v1/correct_sentence_enhanced")
async def correct_sentence_enhanced(request: SentenceCorrectionRequest):
    """
    í–¥ìƒëœ ë¬¸ì¥ êµì • ê¸°ëŠ¥ - AIì™€ ê·œì¹™ ê¸°ë°˜ êµì •ì„ ê²°í•©
    """
    logger.info(f"ğŸ” í–¥ìƒëœ ë¬¸ì¥ êµì • ìš”ì²­ ì²˜ë¦¬ ì¤‘... (í…ìŠ¤íŠ¸: {request.text})")
    
    try:
        # ë¬¸ë§¥ ê°ì§€ (MBTI ì§ˆë¬¸ì¸ì§€ í™•ì¸)
        context = 'general'
        mbti_keywords = ['ìƒê°', 'ëŠë‚Œ', 'ê°ì •', 'ë…¼ë¦¬', 'ì‚¬ì‹¤', 'ê°ê´€', 'ì£¼ê´€']
        if any(keyword in request.text for keyword in mbti_keywords):
            context = 'mbti_question'
        
        # í–¥ìƒëœ ë¬¸ì¥ êµì • ìˆ˜í–‰
        result = await correct_sentence_with_ai_enhanced(request.text, context)
        
        if result["success"]:
            logger.info(f"í–¥ìƒëœ êµì • ê²°ê³¼: '{result['corrected_text']}' (ë°©ë²•: {result['method_used']})")
            return result
        else:
            # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ êµì • ì‹œìŠ¤í…œ ì‚¬ìš©
            logger.warning("í–¥ìƒëœ êµì • ì‹¤íŒ¨, ê¸°ì¡´ ì‹œìŠ¤í…œìœ¼ë¡œ ëŒ€ì²´")
            return await correct_sentence(request)
            
    except Exception as e:
        logger.error(f"í–¥ìƒëœ ë¬¸ì¥ êµì • ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ ê¸°ì¡´ êµì • ì‹œìŠ¤í…œìœ¼ë¡œ ëŒ€ì²´
        return await correct_sentence(request)

@app.post("/audio_quality_check")
@app.post("/api/v1/audio_quality_check")
async def check_audio_quality(audio_file: UploadFile = File(...)):
    """
    ì˜¤ë””ì˜¤ í’ˆì§ˆ ê²€ì¦ ë° ê°œì„  ì œì•ˆ
    """
    logger.info(f"ğŸ” ì˜¤ë””ì˜¤ í’ˆì§ˆ ê²€ì¦ ìš”ì²­ ì²˜ë¦¬ ì¤‘... (íŒŒì¼ëª…: {audio_file.filename})")
    
    try:
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_file:
            content = await audio_file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # ì˜¤ë””ì˜¤ í’ˆì§ˆ ê²€ì¦
            quality_result = validate_audio_quality(temp_file_path)
            
            logger.info(f"ì˜¤ë””ì˜¤ í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: ì ìˆ˜ {quality_result['quality_score']:.2f}")
            
            return {
                "success": True,
                "quality_score": quality_result["quality_score"],
                "duration": quality_result["duration"],
                "rms_energy": quality_result["rms_energy"],
                "issues": quality_result["issues"],
                "suggestions": quality_result["suggestions"],
                "is_good": quality_result["is_good"]
            }
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                
    except Exception as e:
        logger.error(f"ì˜¤ë””ì˜¤ í’ˆì§ˆ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì˜¤ë””ì˜¤ í’ˆì§ˆ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/stt_enhancement_status")
@app.get("/api/v1/stt_enhancement_status")
async def get_stt_enhancement_status():
    """
    STT í–¥ìƒ ê¸°ëŠ¥ ìƒíƒœ í™•ì¸
    """
    try:
        # í–¥ìƒëœ ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        enhanced_modules_available = True
        missing_modules = []
        
        try:
            from mbti_analyzer.modules.stt_module_enhanced import transcribe_audio_file_enhanced
        except ImportError:
            enhanced_modules_available = False
            missing_modules.append("stt_module_enhanced")
        
        try:
            from mbti_analyzer.modules.sentence_correction_enhanced import correct_sentence_with_ai_enhanced
        except ImportError:
            enhanced_modules_available = False
            missing_modules.append("sentence_correction_enhanced")
        
        return {
            "enhanced_features_available": enhanced_modules_available,
            "missing_modules": missing_modules,
            "available_endpoints": [
                "/stt_enhanced",
                "/correct_sentence_enhanced", 
                "/audio_quality_check"
            ] if enhanced_modules_available else [],
            "recommendations": [
                "í–¥ìƒëœ STT ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ /stt_enhanced ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”",
                "í–¥ìƒëœ ë¬¸ì¥ êµì •ì„ ì‚¬ìš©í•˜ë ¤ë©´ /correct_sentence_enhanced ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”",
                "ì˜¤ë””ì˜¤ í’ˆì§ˆì„ ë¯¸ë¦¬ í™•ì¸í•˜ë ¤ë©´ /audio_quality_check ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"
            ] if enhanced_modules_available else [
                "í–¥ìƒëœ ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì„¸ìš”."
            ]
        }
        
    except Exception as e:
        logger.error(f"STT í–¥ìƒ ê¸°ëŠ¥ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "enhanced_features_available": False,
            "error": str(e)
        }

# ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ API ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.post("/api/v1/learning/feedback")
async def submit_learning_feedback(request: LearningFeedbackRequest):
    """í•™ìŠµ í”¼ë“œë°± ì œì¶œ"""
    try:
        result = await learning_manager.process_user_input(
            request.question,
            request.answer,
            request.expected_score,
            request.actual_score
        )
        return {
            "success": True,
            "learning_result": result,
            "message": "í•™ìŠµ í”¼ë“œë°±ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤."
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í•™ìŠµ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/api/v1/learning/status")
async def get_learning_status() -> LearningStatusResponse:
    """í•™ìŠµ ìƒíƒœ ì¡°íšŒ"""
    performance = learning_manager.get_recent_performance()
    return LearningStatusResponse(
        enabled=learning_manager.learning_enabled,
        total_inputs=performance["total_inputs"],
        acceptable_rate=performance["acceptable_rate"],
        average_error=performance["average_error"],
        current_version=learning_manager.current_prompt_version
    )

@app.post("/api/v1/learning/toggle")
async def toggle_learning(enabled: bool = True):
    """í•™ìŠµ ê¸°ëŠ¥ ì¼œê¸°/ë„ê¸°"""
    learning_manager.learning_enabled = enabled
    return {
        "success": True,
        "enabled": learning_manager.learning_enabled,
        "message": f"ì‹¤ì‹œê°„ í•™ìŠµì´ {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}ë˜ì—ˆìŠµë‹ˆë‹¤."
    }

@app.get("/api/v1/learning/history")
async def get_learning_history():
    """í•™ìŠµ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    conn = sqlite3.connect(learning_manager.db_path)
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT question, answer, expected_score, actual_score, error, is_acceptable, timestamp, prompt_version
        FROM user_inputs
        ORDER BY timestamp DESC
        LIMIT 50
    ''')
    
    results = cursor.fetchall()
    conn.close()
    
    history = []
    for row in results:
        history.append({
            "question": row[0],
            "answer": row[1],
            "expected_score": row[2],
            "actual_score": row[3],
            "error": row[4],
            "is_acceptable": bool(row[5]),
            "timestamp": row[6],
            "prompt_version": row[7]
        })
    
    return {
        "success": True,
        "history": history,
        "total_count": len(history)
    }

@app.get("/index_with_learning.html")
async def read_index_with_learning():
    """ì‹¤ì‹œê°„ í•™ìŠµì´ í†µí•©ëœ ë©”ì¸ í˜ì´ì§€"""
    return FileResponse("index_with_learning.html")

@app.post("/tts")
@app.post("/api/v1/tts")
async def text_to_speech_endpoint(request: TextRequest = None, text: str = Form(None)):
    """
    Text to speech endpoint that converts text to audio
    Supports both JSON and FormData requests
    """
    # í…ìŠ¤íŠ¸ ì¶”ì¶œ (JSON ë˜ëŠ” FormData)
    if request and request.text:
        text_content = request.text
    elif text:
        text_content = text
    else:
        logger.error("âŒ í…ìŠ¤íŠ¸ê°€ ì œê³µë˜ì§€ ì•ŠìŒ")
        raise HTTPException(status_code=400, detail="No text provided")
    
    logger.info(f"ğŸ” TTS ìš”ì²­ ì²˜ë¦¬ ì¤‘... (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text_content)})")
    try:
        # ëª¨ë“ˆí™”ëœ TTS í•¨ìˆ˜ ì‚¬ìš© (gTTS)
        audio_path = text_to_speech(
            text=text_content,
            lang='ko-KR',
            voice_name='ko-KR-Chirp3-HD-Leda',
            gender='FEMALE',
            speaking_rate=1.1,
            pitch=0.0
        )
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì‘ë‹µìœ¼ë¡œ ë°˜í™˜
        return FileResponse(
            path=audio_path,
            media_type="audio/mpeg",
            filename="speech.mp3"
        )
        
    except Exception as e:
        print(f"TTS Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def remove_tts_code():
    pass

@app.post("/correct_sentence")
@app.post("/api/v1/correct_sentence")
async def correct_sentence(request: SentenceCorrectionRequest):
    """
    STT ê²°ê³¼ ë¬¸ì¥ì„ êµì •í•©ë‹ˆë‹¤.
    """
    logger.info(f"ğŸ” ë¬¸ì¥ êµì • ìš”ì²­ ì²˜ë¦¬ ì¤‘... (í…ìŠ¤íŠ¸: {request.text})")
    
    try:
        # ë¬¸ì¥ êµì • í”„ë¡¬í”„íŠ¸ ìƒì„± (ë” êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§€ì‹œì‚¬í•­)
        prompt = f"""
ë‹¤ìŒ ìŒì„± ì¸ì‹ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ½ê³  ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ êµì •í•´ì£¼ì„¸ìš”.

êµì • ê·œì¹™:
1. ì˜¤íƒ€ë‚˜ ì˜ëª»ëœ ë‹¨ì–´ë¥¼ ì˜¬ë°”ë¥¸ ë‹¨ì–´ë¡œ ìˆ˜ì •
2. ë¬¸ë²• ì˜¤ë¥˜ë¥¼ ìˆ˜ì • (ì¡°ì‚¬, ì–´ë¯¸ ë“±)
3. ë¶ˆì™„ì „í•œ ë¬¸ì¥ì„ ì™„ì„±
4. ì›ë˜ ì˜ë¯¸ëŠ” ë°˜ë“œì‹œ ìœ ì§€
5. êµì •ëœ ë¬¸ì¥ë§Œ ì¶œë ¥ (ì„¤ëª… ì—†ì´)

ìŒì„± ì¸ì‹ ê²°ê³¼: "{request.text}"

êµì •ëœ ë¬¸ì¥:
"""
        
        # AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ êµì •
        try:
            # Gemini AI ì§ì ‘ ì‚¬ìš©
            import google.generativeai as genai
            from mbti_analyzer.config.settings import settings
            
            # Gemini AI ì„¤ì •
            import os
            gemini_key = os.getenv('GEMINI_API_KEY')
            if not gemini_key:
                # settingsì—ì„œ í™•ì¸
                gemini_key = settings.gemini_api_key
                if not gemini_key:
                    raise Exception("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ GEMINI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            
            logger.info(f"Gemini API í‚¤ í™•ì¸: {gemini_key[:10]}...")
            genai.configure(api_key=gemini_key)
            
            model = genai.GenerativeModel('gemini-1.5-flash')
            
            # AI ì‘ë‹µ ìƒì„±
            response = model.generate_content(prompt)
            corrected_text = response.text.strip()
            
            logger.info(f"AI ì‘ë‹µ ì›ë³¸: {corrected_text}")
            
            # êµì • ê²°ê³¼ ì •ë¦¬
            if corrected_text and isinstance(corrected_text, str):
                # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
                corrected_text = corrected_text.replace("êµì •ëœ ë¬¸ì¥:", "").strip()
                corrected_text = corrected_text.replace("êµì •:", "").strip()
                corrected_text = corrected_text.replace("ìˆ˜ì •ëœ ë¬¸ì¥:", "").strip()
                corrected_text = corrected_text.replace("ê²°ê³¼:", "").strip()
                corrected_text = corrected_text.replace("ë‹µë³€:", "").strip()
                
                # ì¤„ë°”ê¿ˆ ì •ë¦¬
                corrected_text = corrected_text.replace("\n", " ").strip()
                
                # ë”°ì˜´í‘œ ì œê±°
                corrected_text = corrected_text.strip('"').strip("'").strip()
                
                # ë¹ˆ ë¬¸ìì—´ ì²´í¬
                if not corrected_text:
                    corrected_text = request.text
                
                logger.info(f"ì •ë¦¬ëœ êµì • ê²°ê³¼: '{corrected_text}'")
                
                # ì›ë³¸ê³¼ ë™ì¼í•œ ê²½ìš° ì›ë³¸ ë°˜í™˜
                has_changes = corrected_text != request.text
                
                if not has_changes:
                    logger.info("êµì • ê²°ê³¼ê°€ ì›ë³¸ê³¼ ë™ì¼í•¨")
                
                return {
                    "success": True,
                    "original_text": request.text,
                    "corrected_text": corrected_text,
                    "has_changes": has_changes,
                    "ai_response": response.text.strip()  # ë””ë²„ê¹…ìš©
                }
            else:
                raise Exception("AI ë¬¸ì¥ êµì • ìƒì„± ì‹¤íŒ¨")
                
        except Exception as ai_error:
            logger.error(f"AI ë¬¸ì¥ êµì • ì‹¤íŒ¨: {ai_error}")
            # AI ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
            return {
                "success": True,
                "original_text": request.text,
                "corrected_text": request.text,
                "has_changes": False,
                "fallback": True,
                "error": str(ai_error),
                "error_type": type(ai_error).__name__
            }
            
    except Exception as e:
        logger.error(f"ë¬¸ì¥ êµì • ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë¬¸ì¥ êµì • ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

class SummarizeRequest(BaseModel):
    text: str
    type: str  # "detailed_analysis", "reasoning", "suggestions"

@app.post("/api/v1/summarize")
async def summarize_text(request: SummarizeRequest):
    """
    AIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.
    """
    logger.info(f"ğŸ” AI ìš”ì•½ ìš”ì²­ ì²˜ë¦¬ ì¤‘... (íƒ€ì…: {request.type}, í…ìŠ¤íŠ¸ ê¸¸ì´: {len(request.text)})")
    
    try:
        # ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±
        if request.type == "detailed_analysis":
            prompt = f"""
ë‹¤ìŒ MBTI T/F ë¶„ì„ì˜ ìƒì„¸ ë¶„ì„ ë‚´ìš©ì„ 2-3ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

{request.text}

ìš”ì•½ (2-3ì¤„):
"""
        elif request.type == "reasoning":
            prompt = f"""
ë‹¤ìŒ MBTI T/F ë¶„ì„ì˜ ë¶„ì„ ê·¼ê±°ë¥¼ 2-3ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

{request.text}

ìš”ì•½ (2-3ì¤„):
"""
        elif request.type == "suggestions":
            prompt = f"""
ë‹¤ìŒ MBTI T/F ë¶„ì„ì˜ ê°œì„  ì œì•ˆì„ 2-3ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

{request.text}

ìš”ì•½ (2-3ì¤„):
"""
        else:
            raise HTTPException(status_code=400, detail="Invalid type parameter")

        # AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ìƒì„±
        try:
            # Gemini AI ì‚¬ìš© (1ìˆœìœ„)
            from mbti_analyzer.core.question_generator import generate_ai_questions_real
            summary = await generate_ai_questions_real(prompt)
            
            # ìš”ì•½ ê²°ê³¼ ì •ë¦¬
            if summary and isinstance(summary, str):
                # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
                summary = summary.replace("ìš”ì•½ (2-3ì¤„):", "").strip()
                summary = summary.replace("ìš”ì•½:", "").strip()
                
                # ì¤„ë°”ê¿ˆ ì •ë¦¬
                summary = summary.replace("\n\n", "\n").replace("\n", " ")
                
                return {
                    "success": True,
                    "summary": summary,
                    "type": request.type,
                    "original_length": len(request.text),
                    "summary_length": len(summary)
                }
            else:
                raise Exception("AI ìš”ì•½ ìƒì„± ì‹¤íŒ¨")
                
        except Exception as ai_error:
            logger.error(f"AI ìš”ì•½ ì‹¤íŒ¨: {ai_error}")
            # AI ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´
            fallback_summary = request.text[:100] + "..." if len(request.text) > 100 else request.text
            return {
                "success": True,
                "summary": fallback_summary,
                "type": request.type,
                "original_length": len(request.text),
                "summary_length": len(fallback_summary),
                "fallback": True
            }
            
    except Exception as e:
        logger.error(f"ìš”ì•½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ìš”ì•½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    # ì„œë²„ ì‹œì‘ ì‹œ debug.log íŒŒì¼ ì´ˆê¸°í™”
    with open("debug.log", "w", encoding="utf-8") as f:
        f.write("[DEBUG] ì„œë²„ê°€ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤!\n")
        f.write(f"[DEBUG] ì‹¤í–‰ ì¤‘ì¸ íŒŒì¼: {os.path.abspath(__file__)}\n")
    
    # Static íŒŒì¼ ë§ˆìš´íŠ¸
    app.mount("/", StaticFiles(directory=".", html=True), name="static")
    
    uvicorn.run(app, host="0.0.0.0", port=8000) 