"""
T/F ì„±í–¥ ë¶„ì„ ë¡œì§

í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ MBTIì˜ T(ì‚¬ê³ í˜•)/F(ê°ì •í˜•) ì„±í–¥ì„ íŒë‹¨í•˜ëŠ” í•µì‹¬ ë¡œì§ì…ë‹ˆë‹¤.
"""

import re
import logging
from typing import Dict, Optional
import google.generativeai as genai
import os
from groq import AsyncGroq

logger = logging.getLogger(__name__)

def analyze_tf_tendency(text: str) -> float:
    """
    í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ T/F ì„±í–¥ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ T, 100ì— ê°€ê¹Œìš¸ìˆ˜ë¡ F ì„±í–¥ì…ë‹ˆë‹¤.
    """
    text = text.lower()
    logger.info(f"ğŸ” Fallback ë¶„ì„ ì‹œì‘: {text[:50]}...")

    # ì‚¬ê³ í˜•(T) ê°•í•œ ë¬´ì‹¬/ë‹¨ì •/ê°ê´€ì  í‘œí˜„ íŒ¨í„´ (í™•ì‹¤í•œ ì‚¬ê³ í˜•)
    t_strong_patterns = [
        r"ì–´ì©Œë¼ê³ ", r"ìƒê´€ì—†ì–´", r"ì•Œì•„ì„œ í•´", r"ë‚´ ì•Œ ë°” ì•„ëƒ", r"ê·¸ê²Œ ë‚˜ë‘ ë¬´ìŠ¨ ìƒê´€ì´ì•¼",
        r"ë„¤ ë§ˆìŒëŒ€ë¡œ í•´", r"ë‚´ê°€ ë­˜", r"ê·¸ê±´ ë„¤ ë¬¸ì œì•¼", r"ê·¸ê±´ ì¤‘ìš”í•˜ì§€ ì•Šì•„"
    ]
    t_strong_count = sum(len(re.findall(pattern, text)) for pattern in t_strong_patterns)
    if t_strong_count > 0:
        score = max(15, 30 - (t_strong_count - 1) * 5)
        return float(score)

    # ì‹¸ê°€ì§€ ì—†ëŠ”(ê³µê° ì—†ëŠ” í‰ëª…/ë¬´ì‹¬) íŒ¨í„´ (ì‚´ì§ T)
    t_rude_patterns = [
        r"ëª°ë¼", r"ë”±íˆ", r"ë³„ ìƒê° ì—†ì–´", r"ì‹ ê²½ ì•ˆ ì¨", r"ê´€ì‹¬ ì—†ì–´", r"ê·¸ëƒ¥ ê·¸ë˜", r"ê¸€ì„", r"ìŒ[.\.\,\!\?â€¦]*$", r"ë³„ë¡œì•¼"
    ]
    t_rude_count = sum(len(re.findall(pattern, text)) for pattern in t_rude_patterns)
    if t_rude_count > 0:
        # í‰ëª…/ë¬´ì‹¬ íŒ¨í„´ì´ ê°ì§€ë˜ë©´ 35~45ì (ì‚´ì§ T)
        score = max(35, 45 - (t_rude_count - 1) * 3)
        return float(score)

    # 1. í‚¤ì›Œë“œ(í•µì‹¬/ì•½í•œ) ë° ê°€ì¤‘ì¹˜
    t_keywords_strong = [
        'ë…¼ë¦¬', 'ë¶„ì„', 'íŒë‹¨', 'íš¨ìœ¨', 'ê°ê´€', 'ì‚¬ì‹¤', 'ì¦ê±°', 'í•©ë¦¬', 'ì´ì„±', 'ì²´ê³„',
        'ì •í™•', 'ëª…í™•', 'ì¼ê´€', 'ë°ì´í„°', 'í†µê³„', 'ì¸¡ì •',
        'ë§ë‹¤', 'í‹€ë ¸ë‹¤', 'ì •ë‹µ', 'í™•ì‹¤', 'ëª…ë°±', 'ë¶„ëª…', 'í™•ì¸',
        'ê²€í† ', 'í‰ê°€', 'ê¸°ì¤€', 'ì¡°ê±´', 'í•´ê²°', 'ê°œì„ ',
        'ìµœì ', 'íš¨ê³¼', 'ê²°ì •', 'ì„ íƒ', 'ìš°ì„ ìˆœìœ„', 'ì¤‘ìš”ë„',
        'ë¶ˆê°€ëŠ¥', 'ë¬¸ì œ', 'í•´ë‹µ', 'ë‹µ', 'ë°˜ë“œì‹œ', 'ë¬´ì¡°ê±´', 'ì²´í¬', 'ì‹¤ìš©ì ', 'ê³„ì‚°'
    ]
    t_keywords_weak = [
        'ê³„íš', 'ì „ëµ', 'ëª©í‘œ', 'ì„±ê³¼', 'ë°©ë²•', 'í•´ì•¼', 'í•´ì•¼ì§€', 'í•˜ì', 'ëë‹¤', 'ì•ˆ ë¼', 'ì•ˆ ë¨',
        'í™•ì‹¤íˆ', 'ë¶„ëª…íˆ', 'ì •í™•íˆ', 'ë‹¹ì—°íˆ', 'ë°”ë¡œ', 'ë¨¼ì €', 'ìš°ì„ ', 'ì¼ë‹¨', 'ì •ë¦¬', 'íš¨ê³¼ì ', 'íš¨ìœ¨ì ',
        'ê°„ë‹¨', 'ë³µì¡', 'ê°€ëŠ¥', 'ëë‹¤', 'ìš°ì„ ', 'ì¼ë‹¨', 'í¸í•´', 'í¸ë¦¬', 'ì‰½ë‹¤', 'ì–´ë µë‹¤', 'ì‹œê°„', 'ê°€ê²©', 'ë¹„ìš©'
    ]
    f_keywords_strong = [
        'ê°ì •', 'ë§ˆìŒ', 'ê³µê°', 'ë°°ë ¤', 'ì´í•´', 'ì¡°í™”', 'í˜‘ë ¥', 'ê´€ê³„', 'ì†Œí†µ', 'ì¹œë°€',
        'ê°€ì¹˜', 'ì˜ë¯¸', 'ë„ë•', 'ìœ¤ë¦¬', 'ì§€ì›', 'ê²©ë ¤', 'í–‰ë³µ', 'ìŠ¬í”„', 'ê±±ì •', 'ë¯¸ì•ˆ', 'ê³ ë§ˆ', 'ì†Œì¤‘', 'ì‚¬ë‘',
        'ë”°ëœ»', 'í¬ê·¼', 'ì•„ëŠ‘', 'í¸ì•ˆ', 'ì•ˆì‹¬', 'ë“ ë“ ', 'ê¸°ë¶„', 'ëŠë‚Œ', 'ë§ˆìŒê°€ì§', 'ì‹¬ì •',
        'í•¨ê»˜', 'ê°™ì´', 'ì„œë¡œ', 'ìš°ë¦¬ ëª¨ë‘', 'ì¹œêµ¬', 'ê°€ì¡±', 'ì‚¬ëŒë“¤', 'ë™ë£Œë“¤',
        'ì˜ˆë»', 'ê·€ì—¬ì›Œ', 'ì°©í•´', 'ë©‹ì ¸', 'ì¢‹ì•„í•´', 'ì‹«ì–´í•´'
    ]
    f_keywords_weak = [
        'ê¸°ë»', 'ì¦ê±°ì›Œ', 'ì‹ ë‚˜', 'í–‰ë³µí•´', 'ë§Œì¡±', 'ë¿Œë“¯', 'ì†ìƒ', 'ì§œì¦', 'í™”ë‚˜', 'ë‹µë‹µ', 'ë¶ˆì•ˆ', 'ì‹ ê²½ ì“°ì—¬',
        'ìš°ë¦¬', 'ë‹¤í•¨ê»˜', 'í•¨ê»˜ í•˜ì', 'ê°™ì´ í•˜ì', 'ë§ˆìŒì—', 'ë”°ëœ»', 'í¬ê·¼', 'ë³´ê³  ì‹¶ì–´', 'ë§Œë‚˜ê³  ì‹¶ì–´', 'í•˜ê³  ì‹¶ì–´'
    ]

    # ê°€ì¤‘ì¹˜ ì ìš© ì¹´ìš´íŠ¸
    t_count = sum(2 for keyword in t_keywords_strong if keyword in text) + sum(1 for keyword in t_keywords_weak if keyword in text)
    f_count = sum(2 for keyword in f_keywords_strong if keyword in text) + sum(1 for keyword in f_keywords_weak if keyword in text)

    # íŒ¨í„´/ì–´ì¡°/êµ¬ì¡° ë¶„ì„
    f_patterns = [
        r'ì–´ë–»ê²Œ ìƒê°|ì–´ë–¤ ëŠë‚Œ|ê´œì°®ì„ê¹Œ|ì–´ë–¨ê¹Œ|ì¢‹ì„ ê²ƒ ê°™|ë‚˜ì  ê²ƒ ê°™',
        r'í•˜ë©´ ì¢‹ê² |í–ˆìœ¼ë©´|ì¸ ê²ƒ ê°™|ëŠë‚Œì´|ê¸°ë¶„ì´',
        r'í•¨ê»˜|ê°™ì´|ì„œë¡œ|ìš°ë¦¬|ëª¨ë‘|ë‹¤í•¨ê»˜',
        r'ë¯¸ì•ˆ|ê³ ë§ˆì›Œ|ì‚¬ë‘|ì†Œì¤‘|ì•„ê»´|ì±™ê¸°',
        r'ê³µê°|ì´í•´|ìœ„ë¡œ|ê²©ë ¤|ì‘ì›',
        r'ì¢‹ì•„|ì‹«ì–´|ì˜ˆë»|ê·€ì—¬ì›Œ|ì¬ë°Œ|ì§€ë£¨',
        r'ê¸°ë¶„ ì¢‹|ëŠë‚Œ ì¢‹|ë§ˆìŒì—|ë”°ëœ»|í¬ê·¼',
        r'í•˜ê³  ì‹¶ì–´|ê°€ê³  ì‹¶ì–´|ë³´ê³  ì‹¶ì–´|ë§Œë‚˜ê³  ì‹¶ì–´',
        r'ê°™ì´ í•˜ì|í•¨ê»˜ í•˜ì|ìš°ë¦¬ ëª¨ë‘|ë‹¤ ê°™ì´'
    ]
    t_patterns = [
        r'í•´ì•¼ í•œë‹¤|í•´ì•¼ì§€|í•˜ì|í•˜ë©´ ë¼|ë˜ë©´|ì•ˆ ë˜ë©´',
        r'ë‹¹ì—°íˆ|ì •í™•íˆ|ë§ë‹¤|í‹€ë ¸ë‹¤|ì˜³ë‹¤|ê·¸ë¥´ë‹¤|í™•ì‹¤íˆ|ë¶„ëª…íˆ',
        r'íš¨ìœ¨ì |ì²´ê³„ì |ë…¼ë¦¬ì |í•©ë¦¬ì |ê°ê´€ì ',
        r'ì¤‘ìš”í•œ ê±´|í•µì‹¬ì€|ë¬¸ì œëŠ”|í•´ê²°ì±…ì€|ë°©ë²•ì€',
        r'ë¨¼ì €|ìš°ì„ |ì°¨ë¡€ë¡œ|ë‹¨ê³„ë³„ë¡œ|ê³„íšì ìœ¼ë¡œ',
        r'ê·¸ëƒ¥|ë°”ë¡œ|ë¹¨ë¦¬|ì¦‰ì‹œ|ì¼ë‹¨|ìš°ì„ ',
        r'ì•ˆ ë¼|ì•ˆ ë¨|ë˜ë„¤|ëë‹¤|ê°€ëŠ¥|ë¶ˆê°€ëŠ¥',
        r'ì‰½ë‹¤|ì–´ë µë‹¤|ê°„ë‹¨|ë³µì¡|í¸í•´|í¸ë¦¬',
        r'ê³„ì‚°|ë¹„ìš©|ê°€ê²©|ì‹œê°„|íš¨ê³¼|ì‹¤ìš©'
    ]
    f_pattern_count = sum(len(re.findall(pattern, text)) for pattern in f_patterns)
    t_pattern_count = sum(len(re.findall(pattern, text)) for pattern in t_patterns)
    
    soft_tone = len(re.findall(r'ê²ƒ ê°™ì•„|ì¸ ë“¯|ì•„ë§ˆ|í˜¹ì‹œ|ë©´ ì–´ë–¨ê¹Œ|í•˜ë©´ ì¢‹ê² |~ì¸ê°€|~í• ê¹Œ|~ì§€ ì•Šì„ê¹Œ', text))
    firm_tone = len(re.findall(r'ë°˜ë“œì‹œ|ë¬´ì¡°ê±´|í™•ì‹¤íˆ|ë‹¹ì—°íˆ|ëª…ë°±íˆ|ë¶„ëª…íˆ|í•´ì•¼|í•˜ì|ëœë‹¤|ì•ˆ ëœë‹¤', text))
    question_suggestion = len(re.findall(r'\?|í• ê¹Œ|ì–´ë–¨ê¹Œ|ì¢‹ì„ê¹Œ|ì–´ë•Œ|ê´œì°®ì„ê¹Œ', text))
    statement_command = len(re.findall(r'ë‹¤\.|ì´ë‹¤\.|í•˜ì\.|í•´ì•¼\.|ëœë‹¤\.|ì•ˆ ëœë‹¤\.', text))
    
    total_keywords = t_count + f_count
    total_patterns = f_pattern_count + t_pattern_count  
    total_tone = soft_tone + firm_tone
    total_structure = question_suggestion + statement_command
    
    # 2. í‚¤ì›Œë“œ ì ìˆ˜(ê°€ì¤‘ì¹˜ ë°˜ì˜)
    if total_keywords == 0:
        keyword_score = 50
    else:
        t_ratio = t_count / total_keywords if total_keywords > 0 else 0
        f_ratio = f_count / total_keywords if total_keywords > 0 else 0
        if t_ratio > f_ratio:
            intensity = min(t_count, 4)
            keyword_score = 25 - (intensity * 7)  # ê¸°ì¡´ 30ì—ì„œ 25ë¡œ, ê°•ë„ë³„ -7ì ì”©
        elif f_ratio > t_ratio:
            intensity = min(f_count, 4)
            keyword_score = 75 + (intensity * 7)  # ê¸°ì¡´ 70ì—ì„œ 75ë¡œ, ê°•ë„ë³„ +7ì ì”©
        else:
            keyword_score = 50
    
    # 3. íŒ¨í„´/ì–´ì¡°/êµ¬ì¡° ì ìˆ˜(ë™ì /ì• ë§¤ì‹œ ê°€ì¤‘ì¹˜ ì¦ê°€)
    if total_patterns == 0:
        pattern_score = 50
    else:
        t_pattern_ratio = t_pattern_count / total_patterns
        f_pattern_ratio = f_pattern_count / total_patterns
        if t_pattern_ratio > f_pattern_ratio:
            intensity = min(t_pattern_count, 3)
            pattern_score = 30 - (intensity * 5)
        elif f_pattern_ratio > t_pattern_ratio:
            intensity = min(f_pattern_count, 3)
            pattern_score = 70 + (intensity * 5)
        else:
            pattern_score = 50
    
    if total_tone == 0:
        tone_score = 50
    else:
        if firm_tone > soft_tone:
            intensity = min(firm_tone, 2)
            tone_score = 25 - (intensity * 5)
        elif soft_tone > firm_tone:
            intensity = min(soft_tone, 2)
            tone_score = 75 + (intensity * 5)
        else:
            tone_score = 50
    
    if total_structure == 0:
        structure_score = 50
    else:
        if statement_command > question_suggestion:
            structure_score = 30
        elif question_suggestion > statement_command:
            structure_score = 70
        else:
            structure_score = 50
    
    text_length = len(text.replace(' ', ''))
    # ë™ì /ì• ë§¤í• ìˆ˜ë¡ íŒ¨í„´/ì–´ì¡°/êµ¬ì¡° ê°€ì¤‘ì¹˜ ì¦ê°€
    if total_keywords == 0 or abs(t_count - f_count) <= 2:
        keyword_weight = 0.25
        pattern_weight = 0.3
        tone_weight = 0.25
        structure_weight = 0.2
    elif text_length < 15:
        keyword_weight = 0.5
        pattern_weight = 0.2  
        tone_weight = 0.15
        structure_weight = 0.15
    elif text_length < 30:
        keyword_weight = 0.45
        pattern_weight = 0.25
        tone_weight = 0.2
        structure_weight = 0.1
    else:
        keyword_weight = 0.4
        pattern_weight = 0.3
        tone_weight = 0.2
        structure_weight = 0.1
    
    # 4. ìµœì¢… ì ìˆ˜ ê³„ì‚°
    final_score = (
        keyword_score * keyword_weight +
        pattern_score * pattern_weight +
        tone_score * tone_weight +
        structure_score * structure_weight
    )
    
    # 5. ì ìˆ˜ ë²”ìœ„ ì œí•œ (15~85)
    final_score = max(15, min(85, final_score))
    
    logger.info(f"ğŸ” Fallback ë¶„ì„ ì™„ë£Œ: {final_score}ì ")
    return float(final_score)

async def analyze_with_gemini(text: str) -> Optional[float]:
    """Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ T/F ì„±í–¥ ë¶„ì„"""
    try:
        gemini_key = os.getenv('GEMINI_API_KEY')
        if not gemini_key:
            from mbti_analyzer.config.settings import settings
            gemini_key = settings.gemini_api_key
        
        if not gemini_key:
            logger.warning("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        genai.configure(api_key=gemini_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        prompt = f"""
ë‹¤ìŒ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ MBTIì˜ T(ì‚¬ê³ í˜•)/F(ê°ì •í˜•) ì„±í–¥ì„ í‰ê°€í•´ì£¼ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
- T(ì‚¬ê³ í˜•): ë…¼ë¦¬ì , ê°ê´€ì , ë¶„ì„ì , íš¨ìœ¨ì„± ì¤‘ì‹œ
- F(ê°ì •í˜•): ê°ì •ì , ê³µê°ì , ê´€ê³„ ì¤‘ì‹œ, ê°€ì¹˜ ê¸°ë°˜

í…ìŠ¤íŠ¸: "{text}"

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
<TENDENCY>ì ìˆ˜</TENDENCY>
<REASONING>ë¶„ì„ ê·¼ê±°</REASONING>

ì ìˆ˜ëŠ” 0-100 ì‚¬ì´ì˜ ìˆ«ìë¡œ, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ T ì„±í–¥, 100ì— ê°€ê¹Œìš¸ìˆ˜ë¡ F ì„±í–¥ì…ë‹ˆë‹¤.
"""
        
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        
        # ì ìˆ˜ ì¶”ì¶œ
        import re
        tendency_match = re.search(r'<TENDENCY>(\d+(?:\.\d+)?)</TENDENCY>', response_text)
        if tendency_match:
            score = float(tendency_match.group(1))
            return score
        
        return None
        
    except Exception as e:
        logger.error(f"Gemini AI ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None

async def analyze_with_groq(text: str) -> Optional[float]:
    """Groq AIë¥¼ ì‚¬ìš©í•˜ì—¬ T/F ì„±í–¥ ë¶„ì„"""
    try:
        groq_key = os.getenv('GROQ_API_KEY')
        if not groq_key:
            from mbti_analyzer.config.settings import settings
            groq_key = settings.groq_api_key
        
        if not groq_key:
            logger.warning("Groq API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        client = AsyncGroq(api_key=groq_key)
        
        prompt = f"""
ë‹¤ìŒ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ MBTIì˜ T(ì‚¬ê³ í˜•)/F(ê°ì •í˜•) ì„±í–¥ì„ í‰ê°€í•´ì£¼ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
- T(ì‚¬ê³ í˜•): ë…¼ë¦¬ì , ê°ê´€ì , ë¶„ì„ì , íš¨ìœ¨ì„± ì¤‘ì‹œ
- F(ê°ì •í˜•): ê°ì •ì , ê³µê°ì , ê´€ê³„ ì¤‘ì‹œ, ê°€ì¹˜ ê¸°ë°˜

í…ìŠ¤íŠ¸: "{text}"

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
<TENDENCY>ì ìˆ˜</TENDENCY>
<REASONING>ë¶„ì„ ê·¼ê±°</REASONING>

ì ìˆ˜ëŠ” 0-100 ì‚¬ì´ì˜ ìˆ«ìë¡œ, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ T ì„±í–¥, 100ì— ê°€ê¹Œìš¸ìˆ˜ë¡ F ì„±í–¥ì…ë‹ˆë‹¤.
"""
        
        response = await client.chat.completions.create(
            model="llama3-70b-8192",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=500
        )
        
        response_text = response.choices[0].message.content.strip()
        
        # ì ìˆ˜ ì¶”ì¶œ
        import re
        tendency_match = re.search(r'<TENDENCY>(\d+(?:\.\d+)?)</TENDENCY>', response_text)
        if tendency_match:
            score = float(tendency_match.group(1))
            return score
        
        return None
        
    except Exception as e:
        logger.error(f"Groq AI ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None

async def analyze_text(text: str) -> Dict:
    """í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ T/F ì„±í–¥ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    logger.info(f"ğŸ” ì…ë ¥ í…ìŠ¤íŠ¸: {text.strip()}")
    
    # 1. Gemini AI ì‹œë„
    logger.info("ğŸ” Gemini AI ë¶„ì„ ì‹œë„ ì¤‘...")
    try:
        gemini_result = await analyze_with_gemini(text)
        if gemini_result is not None:
            return {
                "score": gemini_result,
                "method": "gemini",
                "success": True
            }
    except Exception as e:
        logger.info(f"âŒ Gemini AI ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    # 2. Groq AI ì‹œë„
    logger.info("ğŸ” Groq AI ë¶„ì„ ì‹œë„ ì¤‘...")
    try:
        groq_result = await analyze_with_groq(text)
        if groq_result is not None:
            return {
                "score": groq_result,
                "method": "groq",
                "success": True
            }
    except Exception as e:
        logger.info(f"âŒ Groq AI ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    # 3. Fallback ë¶„ì„
    logger.info("ğŸ” Fallback ë¶„ì„ í•¨ìˆ˜ ì‚¬ìš© ì¤‘...")
    fallback_score = analyze_tf_tendency(text)
    
    return {
        "score": fallback_score,
        "method": "fallback",
        "success": True
    } 